{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SharmaAshwini/UNT/blob/main/Methods%20in%20Empirical%20Analysis/Exercise_2_Text_Summarization_TFIDF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import all necessary libraries"
      ],
      "metadata": {
        "id": "xh59A7i2pSuC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uDsazt9jpAou",
        "outputId": "95adf31b-fb6d-4afd-cf69-c2fb1a89ba16"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.cluster.util import cosine_distance\n",
        "import numpy as np\n",
        "import networkx as nx\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate clean sentences (Tutorial)"
      ],
      "metadata": {
        "id": "nzxQax2rpU5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "     #print(sentence)\n",
        "     sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "    #print(sentences)\n",
        "    return sentences"
      ],
      "metadata": {
        "id": "CqWp73ylpQX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read and perform text cleanup (not from Tutorial)"
      ],
      "metadata": {
        "id": "3wEg7L6-uQbA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read Text"
      ],
      "metadata": {
        "id": "btNtODL4C_hZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_article_all_cleanup(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "\n",
        "    for sentence in article:\n",
        "     sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop()\n",
        "\n",
        "    ##Change case to lower\n",
        "    sentences_lower = [[string.lower() for string in sublist] for sublist in sentences]\n",
        "\n",
        "    ##Punctuation\n",
        "    punctuation_signs = list(\"?:!.,;\")\n",
        "    for punct_sign in punctuation_signs:\n",
        "      sentences_lower_punct = [[string.replace(punct_sign, '') for string in sublist] for sublist in sentences_lower]\n",
        "\n",
        "    ##Remove possessive pronoun terminations\n",
        "    sentences_lower_punct_pro = [[string.replace(\"'s\", \"\") for string in sublist] for sublist in sentences_lower_punct]\n",
        "\n",
        "    ##Lemmatization\n",
        "    # Saving the lemmatizer into an object\n",
        "    wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "    lemmatized_text_list = []\n",
        "    for i in range(len(sentences_lower_punct)):\n",
        "      lemmatized_list = []\n",
        "      for j in range(len(sentences_lower_punct[i])):\n",
        "        lemmatized_list.append(wordnet_lemmatizer.lemmatize(sentences_lower_punct[i][j], pos=\"v\"))\n",
        "\n",
        "      # Append to the list containing the texts\n",
        "      lemmatized_text_list.append(lemmatized_list)\n",
        "\n",
        "    return sentences, sentences_lower, sentences_lower_punct, sentences_lower_punct_pro, lemmatized_text_list"
      ],
      "metadata": {
        "id": "uqHmQApJ7DE-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Read from File"
      ],
      "metadata": {
        "id": "Tl3HtBcmDBxK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_clean_file(file_name):\n",
        "  file = open(file_name, \"r\")\n",
        "  filedata = file.readlines()\n",
        "  artc = filedata[0].split(\". \")\n",
        "  length_file = len(artc)\n",
        "  statement = []\n",
        "  for i in range(length_file):\n",
        "    article = artc[i]\n",
        "    statement.append(article.split())\n",
        "\n",
        "  ##Remove \\r, \\n and nulls, generate list of lists\n",
        "  for i in range(len(statement)):\n",
        "    for j in range(len(statement[i])):\n",
        "      if statement[i][j] in [\"\\r\", \"\\n\",\"\",\"    \"]:\n",
        "        statement[i][j] = \" \"\n",
        "      if statement[i][j] in ['\"']:\n",
        "        statement[i][j] = \"\"\n",
        "\n",
        "    while(\" \" in statement[i]):\n",
        "      statement[i].remove(\" \")\n",
        "\n",
        "  while [] in statement:\n",
        "    statement.remove([])\n",
        "\n",
        "  ##Change case to lower\n",
        "  statement = [[string.lower() for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Punctuation\n",
        "  punctuation_signs = list(\"?:!.,;\")\n",
        "  for punct_sign in punctuation_signs:\n",
        "    statement = [[string.replace(punct_sign, '') for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Remove possessive pronoun terminations\n",
        "  statement = [[string.replace(\"'s\", \"\") for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Lemmatization\n",
        "  # Saving the lemmatizer into an object\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_text_list = []\n",
        "  for i in range(len(statement)):\n",
        "    lemmatized_list = []\n",
        "    for j in range(len(statement[i])):\n",
        "      lemmatized_list.append(wordnet_lemmatizer.lemmatize(statement[i][j], pos=\"v\"))\n",
        "\n",
        "    # Append to the list containing the texts\n",
        "    lemmatized_text_list.append(lemmatized_list)\n",
        "\n",
        "  return lemmatized_text_list"
      ],
      "metadata": {
        "id": "6QH4pJtBuQuD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Read and perform text cleanup (not from Tutorial; can handle text files with new lines)"
      ],
      "metadata": {
        "id": "TxptIGrUpZkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def read_and_clean_file_nl(file_name):\n",
        "  file = open(file_name, \"r\")\n",
        "  filedata = file.readlines()\n",
        "  article = filedata[0].split(\". \")\n",
        "  length_file = len(filedata)\n",
        "  statement = []\n",
        "  for i in range(length_file):\n",
        "    article = filedata[i]\n",
        "    statement.append(article.split())\n",
        "\n",
        "  ##Remove \\r, \\n and nulls, generate list of lists\n",
        "  for i in range(len(statement)):\n",
        "    for j in range(len(statement[i])):\n",
        "      if statement[i][j] in [\"\\r\", \"\\n\",\"\",\"    \"]:\n",
        "        statement[i][j] = \" \"\n",
        "      if statement[i][j] in ['\"']:\n",
        "        statement[i][j] = \"\"\n",
        "\n",
        "    while(\" \" in statement[i]):\n",
        "      statement[i].remove(\" \")\n",
        "\n",
        "  while [] in statement:\n",
        "    statement.remove([])\n",
        "\n",
        "  ##Change case to lower\n",
        "  statement = [[string.lower() for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Punctuation\n",
        "  punctuation_signs = list(\"?:!.,;\")\n",
        "  for punct_sign in punctuation_signs:\n",
        "    statement = [[string.replace(punct_sign, '') for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Remove possessive pronoun terminations\n",
        "  statement = [[string.replace(\"'s\", \"\") for string in sublist] for sublist in statement]\n",
        "\n",
        "  ##Lemmatization\n",
        "  # Saving the lemmatizer into an object\n",
        "  wordnet_lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "  lemmatized_text_list = []\n",
        "  for i in range(len(statement)):\n",
        "    lemmatized_list = []\n",
        "    for j in range(len(statement[i])):\n",
        "      lemmatized_list.append(wordnet_lemmatizer.lemmatize(statement[i][j], pos=\"v\"))\n",
        "\n",
        "    # Append to the list containing the texts\n",
        "    lemmatized_text_list.append(lemmatized_list)\n",
        "\n",
        "  return lemmatized_text_list"
      ],
      "metadata": {
        "id": "jVfq9lsPpi9c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Similarity matrix"
      ],
      "metadata": {
        "id": "nMpo0dAhpoEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        "\n",
        "    sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "\n",
        "    # build the vector for the first sentence\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1\n",
        "\n",
        "    # build the vector for the second sentence\n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1\n",
        "\n",
        "    return 1 - cosine_distance(vector1, vector2)"
      ],
      "metadata": {
        "id": "JVGpVlV9prq1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
        "\n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                continue\n",
        "            similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "    return similarity_matrix"
      ],
      "metadata": {
        "id": "kwdhlhGppuhY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Summary Method (Tutorial)"
      ],
      "metadata": {
        "id": "J26jk-dXpwXa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text and tokenize\n",
        "    sentences =  read_article(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
      ],
      "metadata": {
        "id": "s1nfPI0PpxWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Summary Method (with cleanUp)"
      ],
      "metadata": {
        "id": "Pcwa2K5aqahw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_clean_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text and tokenize\n",
        "    sentences =  read_and_clean_file(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
      ],
      "metadata": {
        "id": "y2k7NZpaqdda"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Summary Method (with all cleanUp outputs)"
      ],
      "metadata": {
        "id": "8Uqj7SoXHnBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_all_clean_summary(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    summarize_text_lower = []\n",
        "    summarize_text_lower_punct = []\n",
        "    summarize_text_lower_punct_pro = []\n",
        "    summarize_text_lemmatized = []\n",
        "\n",
        "    # Step 1 - Read text and tokenize\n",
        "    sentences, sentences_lower, sentences_lower_punct, sentences_lower_punct_pro, lemmatized_text_list =  read_article_all_cleanup(file_name)\n",
        "\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "    sentence_similarity_martix_lower = build_similarity_matrix(sentences_lower, stop_words)\n",
        "    sentence_similarity_martix_lower_punct = build_similarity_matrix(sentences_lower_punct, stop_words)\n",
        "    sentence_similarity_martix_lower_punct_pro = build_similarity_matrix(sentences_lower_punct_pro, stop_words)\n",
        "    sentence_similarity_martix_lemmatized = build_similarity_matrix(lemmatized_text_list, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    sentence_similarity_graph_lower = nx.from_numpy_array(sentence_similarity_martix_lower)\n",
        "    scores_lower = nx.pagerank(sentence_similarity_graph_lower)\n",
        "\n",
        "    sentence_similarity_graph_lower_punct = nx.from_numpy_array(sentence_similarity_martix_lower_punct)\n",
        "    scores_lower_punct = nx.pagerank(sentence_similarity_graph_lower_punct)\n",
        "\n",
        "    sentence_similarity_graph_lower_punct_pro = nx.from_numpy_array(sentence_similarity_martix_lower_punct_pro)\n",
        "    scores_lower_punct_pro = nx.pagerank(sentence_similarity_graph_lower_punct_pro)\n",
        "\n",
        "    sentence_similarity_graph_lemmatized = nx.from_numpy_array(sentence_similarity_martix_lemmatized)\n",
        "    scores_lemmatized = nx.pagerank(sentence_similarity_graph_lemmatized)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    ranked_sentence_lower = sorted(((scores_lower[i],s) for i,s in enumerate(sentences_lower)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence (with lower case) order are \", ranked_sentence_lower)\n",
        "\n",
        "    ranked_sentence_lower_punct = sorted(((scores_lower_punct[i],s) for i,s in enumerate(sentences_lower_punct)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence (with lower case & punctuation) order are \", ranked_sentence_lower_punct)\n",
        "\n",
        "    ranked_sentence_lower_punct_pro = sorted(((scores_lower_punct_pro[i],s) for i,s in enumerate(sentences_lower_punct_pro)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence (with lower case, punctuation & pronoun) order are \", ranked_sentence_lower_punct_pro)\n",
        "\n",
        "    ranked_sentence_lemmatized = sorted(((scores_lemmatized[i],s) for i,s in enumerate(lemmatized_text_list)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence (with lower case, punctuation, pronoun & lemmatization) order are \", ranked_sentence_lemmatized)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "      summarize_text_lower.append(\" \".join(ranked_sentence_lower[i][1]))\n",
        "      summarize_text_lower_punct.append(\" \".join(ranked_sentence_lower_punct[i][1]))\n",
        "      summarize_text_lower_punct_pro.append(\" \".join(ranked_sentence_lower_punct_pro[i][1]))\n",
        "      summarize_text_lemmatized.append(\" \".join(ranked_sentence_lemmatized[i][1]))\n",
        "\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text), \"\\n\")\n",
        "    print(\"Summarize Text (with lower case): \\n\", \". \".join(summarize_text_lower), \"\\n\")\n",
        "    print(\"Summarize Text (with lower case & punctuation): \\n\", \". \".join(summarize_text_lower_punct), \"\\n\")\n",
        "    print(\"Summarize Text (with lower case, punctuation & pronoun): \\n\", \". \".join(summarize_text_lower_punct_pro), \"\\n\")\n",
        "    print(\"Summarize Text (with lower case, punctuation, pronoun & lemmatization): \\n\", \". \".join(summarize_text_lemmatized), \"\\n\")"
      ],
      "metadata": {
        "id": "nSUN4A7aHZMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Summary Method (with blogs cleanUp)"
      ],
      "metadata": {
        "id": "z1SRj1YRxg-U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_clean_summary_blog(file_name, top_n=5):\n",
        "    stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "\n",
        "    # Step 1 - Read text and tokenize\n",
        "    sentences =  read_and_clean_file_nl(file_name)\n",
        "\n",
        "    # Step 2 - Generate Similary Martix across sentences\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)\n",
        "\n",
        "    # Step 3 - Rank sentences in similarity martix\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "\n",
        "    # Step 4 - Sort the rank and pick top sentences\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)\n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)\n",
        "\n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        "\n",
        "    # Step 5 - Offcourse, output the summarize texr\n",
        "    print(\"Summarize Text: \\n\", \". \".join(summarize_text))"
      ],
      "metadata": {
        "id": "RJYbHEN2xhSm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test data"
      ],
      "metadata": {
        "id": "VvaCrtdtp13t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"In an attempt to build an AI-ready workforce, Microsoft announced Intelligent Cloud Hub which has been launched to empower the next generation of students with AI-ready skills. Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. As part of the program, the Redmond giant which wants to expand its reach and is planning to build a strong developer ecosystem in India with the program will set up the core AI infrastructure and IoT Hub for the selected campuses. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, \"With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset. This will require more collaborations and training and working with AI. That’s why it has become more critical than ever for educational institutions to integrate new cloud and AI technologies. The program is an attempt to ramp up the institutional set-up and build capabilities among the educators to educate the workforce of tomorrow.\" The program aims to build up the cognitive skills and in-depth understanding of developing intelligent cloud connected solutions for applications across industry. Earlier in April this year, the company announced Microsoft Professional Program In AI as a learning track open to the public. The program was developed to provide job ready skills to programmers who wanted to hone their skills in AI and data science with a series of online courses which featured hands-on labs and expert instructors as well. This program also included developer-focused AI school that provided a bunch of assets to help build AI skills.\" > msft.txt"
      ],
      "metadata": {
        "id": "MqfJWh4tp3bQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"For years, Facebook gave some of the world's largest technology companies more intrusive access to users' personal data than it has disclosed, effectively exempting those business partners from its usual privacy rules, according to internal records and interviews. The special arrangements are detailed in hundreds of pages of Facebook documents obtained by The New York Times. The records, generated in 2017 by the company's internal system for tracking partnerships, provide the most complete picture yet of the social network's data-sharing practices. They also underscore how personal data has become the most prized commodity of the digital age, traded on a vast scale by some of the most powerful companies in Silicon Valley and beyond. The exchange was intended to benefit everyone. Pushing for explosive growth, Facebook got more users, lifting its advertising revenue. Partner companies acquired features to make their products more attractive. Facebook users connected with friends across different devices and websites. But Facebook also assumed extraordinary power over the personal information of its 2 billion users - control it has wielded with little transparency or outside oversight.Facebook allowed Microsoft's Bing search engine to see the names of virtually all Facebook user's friends without consent, the records show, and gave Netflix and Spotify the ability to read Facebook users' private messages.\" > fb.txt"
      ],
      "metadata": {
        "id": "2zmyf4k8p5LI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who said they had not been briefed on the plans.President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he decided to pull American forces out of Syria, one official said.The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after disagreeing with the president over his approach to policy in the Middle East.The whirlwind of troop withdrawals and the resignation of Mr. Mattis leave a murky picture for what is next in the United States’ longest war, and they come as Afghanistan has been troubled by spasms of violence afflicting the capital, Kabul, and other important areas. The United States has also been conducting talks with representatives of the Taliban, in what officials have described as discussions that could lead to formal talks to end the conflict.Senior Afghan officials and Western diplomats in Kabul woke up to the shock of the news on Friday morning, and many of them braced for chaos ahead. Several Afghan officials, often in the loop on security planning and decision-making, said they had received no indication in recent days that the Americans would pull troops out. The fear that Mr. Trump might take impulsive actions, however, often loomed in the background of discussions with the United States, they said.They saw the abrupt decision as a further sign that voices from the ground were lacking in the debate over the war and that with Mr. Mattis’s resignation, Afghanistan had lost one of the last influential voices in Washington who channeled the reality of the conflict into the White House’s deliberations.The president long campaigned on bringing troops home, but in 2017, at the request of Mr. Mattis, he begrudgingly pledged an additional 4,000 troops to the Afghan campaign to try to hasten an end to the conflict.Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for a withdrawal. But with plans to quickly reduce the number of American troops in the country, it is unclear if the Afghans can hold their own against an increasingly aggressive Taliban.Currently, American airstrikes are at levels not seen since the height of the war, when tens of thousands of American troops were spread throughout the country. That air support, officials say, consists mostly of propping up Afghan troops while they try to hold territory from a resurgent Taliban.\" > trump.txt"
      ],
      "metadata": {
        "id": "eib5_8crp6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!echo \"The biggest rumor heading into this year's WWDC is of course the Apple headset. Rumored to be running on a new \"xrOS,\" the device could utilize mixed reality, a combination of virtual reality and augmented reality. There may be eye and hand tracking, high-resolution displays and... a potential $3,000 price tag. Bloomberg's Mark Gurman recently detailed how Apple plans to incorporate sports, gaming, workouts and iPad apps into the headset to show off what the new platform can do. Whether that's enough to excite consumers and persuade them to drop three grand or for developers to commit to building apps for it remains to be seen. The MacBook Air has long been one of Apple's most popular laptops. Frequently sold with a 13-inch screen, Apple has experimented with different sizes of Airs in the past, including offering an 11-inch model for years. Rumors these days, however, suggest that the company has a larger, 15-inch M2-powered Air raring to go. That once again comes from Bloomberg's Gurman, who expects the new laptop to be announced at this year's event. It's about time. While rumors point to an imminent announcement, it's unclear how much Apple might charge for the new Air or how it might fit into the company's existing MacBook lineup. The 2020 M1-powered 13.3-inch MacBook Air is still sold for $999, while the updated M2-powered 2022 13.6-inch MacBook Air starts at $1,199. A 16-inch MacBook Pro, meanwhile, starts at $2,499. Might the 15-inch Air fit somewhere in the middle? Oh, the Mac Pro. Apple last updated the Mac Pro at WWDC in 2019. Despite some teases that confirmed it's working on a new one powered by its Apple Silicon chips, the company has largely been quiet about the super powerful computer. Might the \"another day\" be June 5? It's possible and Mac Pro fans may want to tune in, but with tempered expectations. In an April appearance on The MacRumors Show, Gurman, the Apple savant, suggests that it still may arrive this year but not at WWDC. In addition to all the hardware rumors, we can expect Apple to detail the latest updates coming this year to its iOS, iPadOS, WatchOS and TVOS platforms. Among the bigger iOS changes, Apple might finally add support for installing apps not downloaded from the App Store. The iPhone maker has long resisted opening up its mobile software to allow for sideloading, but new European regulations may have forced its hand. Other software changes Apple might unveil include a new mental health app as well as widgets returning to the Apple Watch.\" > apple_mwc.txt"
      ],
      "metadata": {
        "id": "YxRgVWFOqDFr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Test Runs"
      ],
      "metadata": {
        "id": "K6nK8mbDpwRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### generate summary function is from tutorial and generate_clean_summary has text cleanup"
      ],
      "metadata": {
        "id": "N3X5Ndn80TjP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test from tutorial and with CleanUp"
      ],
      "metadata": {
        "id": "ciNkEDxzEgnQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary( \"msft.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UorKUoV7qSn9",
        "outputId": "0a5d0c10-22f4-4091-c7a8-67ddde031f5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.1357055978868266, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.12193277895857677, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', 'With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10556270791591098, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.10502570031914885, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10295318634280672, ['The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09892112780947322, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.08997641404848532, ['That’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.08780388953142723, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08709908967646102, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI']), (0.06501950751088326, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Summarize Text: \n",
            " Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary(\"msft.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4YCvkjycqzH4",
        "outputId": "90499e91-21a4-4af6-f60e-1794d3e2f9b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.13129671766785161, ['this', 'program', 'also', 'include', 'developer-focused', 'ai', 'school', 'that', 'provide', 'a', 'bunch', 'of', 'assets', 'to', 'help', 'build', 'ai', 'skills']), (0.11877677193211429, ['envision', 'as', 'a', 'three-year', 'collaborative', 'program', 'intelligent', 'cloud', 'hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'ai', 'infrastructure', 'course', 'content', 'and', 'curriculum', 'developer', 'support', 'development', 'tool', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'ai', 'service']), (0.09870938464353562, ['as', 'part', 'of', 'the', 'program', 'the', 'redmond', 'giant', 'which', 'want', 'to', 'expand', 'its', 'reach', 'and', 'be', 'plan', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'india', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'ai', 'infrastructure', 'and', 'iot', 'hub', 'for', 'the', 'select', 'campuses']), (0.09741236591123935, ['the', 'company', 'will', 'provide', 'ai', 'development', 'tool', 'and', 'azure', 'ai', 'service', 'such', 'as', 'microsoft', 'cognitive', 'service', 'bot', 'service', 'and', 'azure', 'machine', 'learningaccording', 'to', 'manish', 'prakash', 'country', 'general', 'manager-ps', 'health', 'and', 'education', 'microsoft', 'india', 'say', 'with', 'ai', 'be', 'the', 'define', 'technology', 'of', 'our', 'time', 'it', 'be', 'transform', 'live', 'and', 'industry', 'and', 'the', 'job', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.08997526515238427, ['the', 'program', 'be', 'develop', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'want', 'to', 'hone', 'their', 'skills', 'in', 'ai', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'course', 'which', 'feature', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08867050539922691, ['the', 'program', 'aim', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understand', 'of', 'develop', 'intelligent', 'cloud', 'connect', 'solutions', 'for', 'applications', 'across', 'industry']), (0.0883001356383904, ['earlier', 'in', 'april', 'this', 'year', 'the', 'company', 'announce', 'microsoft', 'professional', 'program', 'in', 'ai', 'as', 'a', 'learn', 'track', 'open', 'to', 'the', 'public']), (0.08185253258942052, ['in', 'an', 'attempt', 'to', 'build', 'an', 'ai-ready', 'workforce', 'microsoft', 'announce', 'intelligent', 'cloud', 'hub', 'which', 'have', 'be', 'launch', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'ai-ready', 'skills']), (0.07185736104784766, ['this', 'will', 'require', 'more', 'collaborations', 'and', 'train', 'and', 'work', 'with', 'ai']), (0.06990251602565181, ['that’s', 'why', 'it', 'have', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'ai', 'technologies']), (0.06324644399233766, ['the', 'program', 'be', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Summarize Text: \n",
            " this program also include developer-focused ai school that provide a bunch of assets to help build ai skills. envision as a three-year collaborative program intelligent cloud hub will support around 100 institutions with ai infrastructure course content and curriculum developer support development tool and give students access to cloud and ai service\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test for different CleanUps"
      ],
      "metadata": {
        "id": "dy-Y2XtuElIs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_all_clean_summary(\"msft.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxDB3hs8Jj5g",
        "outputId": "539a5192-fb3b-47a7-cef3-89da0bd7b9ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.1357055978868266, ['Envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'Intelligent', 'Cloud', 'Hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'AI', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'AI', 'services']), (0.12193277895857677, ['The', 'company', 'will', 'provide', 'AI', 'development', 'tools', 'and', 'Azure', 'AI', 'services', 'such', 'as', 'Microsoft', 'Cognitive', 'Services,', 'Bot', 'Services', 'and', 'Azure', 'Machine', 'Learning.According', 'to', 'Manish', 'Prakash,', 'Country', 'General', 'Manager-PS,', 'Health', 'and', 'Education,', 'Microsoft', 'India,', 'said,', 'With', 'AI', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10556270791591098, ['In', 'an', 'attempt', 'to', 'build', 'an', 'AI-ready', 'workforce,', 'Microsoft', 'announced', 'Intelligent', 'Cloud', 'Hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'AI-ready', 'skills']), (0.10502570031914885, ['Earlier', 'in', 'April', 'this', 'year,', 'the', 'company', 'announced', 'Microsoft', 'Professional', 'Program', 'In', 'AI', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10295318634280672, ['The', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09892112780947322, ['As', 'part', 'of', 'the', 'program,', 'the', 'Redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'India', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'AI', 'infrastructure', 'and', 'IoT', 'Hub', 'for', 'the', 'selected', 'campuses']), (0.08997641404848532, ['That’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'AI', 'technologies']), (0.08780388953142723, ['The', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'AI', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08709908967646102, ['This', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'AI']), (0.06501950751088326, ['The', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Indexes of top ranked_sentence (with lower case) order are  [(0.1357055978868266, ['envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'intelligent', 'cloud', 'hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'ai', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'ai', 'services']), (0.12193277895857677, ['the', 'company', 'will', 'provide', 'ai', 'development', 'tools', 'and', 'azure', 'ai', 'services', 'such', 'as', 'microsoft', 'cognitive', 'services,', 'bot', 'services', 'and', 'azure', 'machine', 'learning.according', 'to', 'manish', 'prakash,', 'country', 'general', 'manager-ps,', 'health', 'and', 'education,', 'microsoft', 'india,', 'said,', 'with', 'ai', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10556270791591098, ['in', 'an', 'attempt', 'to', 'build', 'an', 'ai-ready', 'workforce,', 'microsoft', 'announced', 'intelligent', 'cloud', 'hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'ai-ready', 'skills']), (0.10502570031914885, ['earlier', 'in', 'april', 'this', 'year,', 'the', 'company', 'announced', 'microsoft', 'professional', 'program', 'in', 'ai', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10295318634280672, ['the', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09892112780947322, ['as', 'part', 'of', 'the', 'program,', 'the', 'redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'india', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'ai', 'infrastructure', 'and', 'iot', 'hub', 'for', 'the', 'selected', 'campuses']), (0.08997641404848532, ['that’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'ai', 'technologies']), (0.08780388953142723, ['the', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'ai', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08709908967646102, ['this', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'ai']), (0.06501950751088326, ['the', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Indexes of top ranked_sentence (with lower case & punctuation) order are  [(0.1357055978868266, ['envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'intelligent', 'cloud', 'hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'ai', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'ai', 'services']), (0.12193277895857677, ['the', 'company', 'will', 'provide', 'ai', 'development', 'tools', 'and', 'azure', 'ai', 'services', 'such', 'as', 'microsoft', 'cognitive', 'services,', 'bot', 'services', 'and', 'azure', 'machine', 'learning.according', 'to', 'manish', 'prakash,', 'country', 'general', 'manager-ps,', 'health', 'and', 'education,', 'microsoft', 'india,', 'said,', 'with', 'ai', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10556270791591098, ['in', 'an', 'attempt', 'to', 'build', 'an', 'ai-ready', 'workforce,', 'microsoft', 'announced', 'intelligent', 'cloud', 'hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'ai-ready', 'skills']), (0.10502570031914885, ['earlier', 'in', 'april', 'this', 'year,', 'the', 'company', 'announced', 'microsoft', 'professional', 'program', 'in', 'ai', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10295318634280672, ['the', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09892112780947322, ['as', 'part', 'of', 'the', 'program,', 'the', 'redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'india', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'ai', 'infrastructure', 'and', 'iot', 'hub', 'for', 'the', 'selected', 'campuses']), (0.08997641404848532, ['that’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'ai', 'technologies']), (0.08780388953142723, ['the', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'ai', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08709908967646102, ['this', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'ai']), (0.06501950751088326, ['the', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Indexes of top ranked_sentence (with lower case, punctuation & pronoun) order are  [(0.1357055978868266, ['envisioned', 'as', 'a', 'three-year', 'collaborative', 'program,', 'intelligent', 'cloud', 'hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'ai', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tools', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'ai', 'services']), (0.12193277895857677, ['the', 'company', 'will', 'provide', 'ai', 'development', 'tools', 'and', 'azure', 'ai', 'services', 'such', 'as', 'microsoft', 'cognitive', 'services,', 'bot', 'services', 'and', 'azure', 'machine', 'learning.according', 'to', 'manish', 'prakash,', 'country', 'general', 'manager-ps,', 'health', 'and', 'education,', 'microsoft', 'india,', 'said,', 'with', 'ai', 'being', 'the', 'defining', 'technology', 'of', 'our', 'time,', 'it', 'is', 'transforming', 'lives', 'and', 'industry', 'and', 'the', 'jobs', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10556270791591098, ['in', 'an', 'attempt', 'to', 'build', 'an', 'ai-ready', 'workforce,', 'microsoft', 'announced', 'intelligent', 'cloud', 'hub', 'which', 'has', 'been', 'launched', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'ai-ready', 'skills']), (0.10502570031914885, ['earlier', 'in', 'april', 'this', 'year,', 'the', 'company', 'announced', 'microsoft', 'professional', 'program', 'in', 'ai', 'as', 'a', 'learning', 'track', 'open', 'to', 'the', 'public']), (0.10295318634280672, ['the', 'program', 'aims', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understanding', 'of', 'developing', 'intelligent', 'cloud', 'connected', 'solutions', 'for', 'applications', 'across', 'industry']), (0.09892112780947322, ['as', 'part', 'of', 'the', 'program,', 'the', 'redmond', 'giant', 'which', 'wants', 'to', 'expand', 'its', 'reach', 'and', 'is', 'planning', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'india', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'ai', 'infrastructure', 'and', 'iot', 'hub', 'for', 'the', 'selected', 'campuses']), (0.08997641404848532, ['that’s', 'why', 'it', 'has', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'ai', 'technologies']), (0.08780388953142723, ['the', 'program', 'was', 'developed', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'wanted', 'to', 'hone', 'their', 'skills', 'in', 'ai', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'courses', 'which', 'featured', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08709908967646102, ['this', 'will', 'require', 'more', 'collaborations', 'and', 'training', 'and', 'working', 'with', 'ai']), (0.06501950751088326, ['the', 'program', 'is', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Indexes of top ranked_sentence (with lower case, punctuation, pronoun & lemmatization) order are  [(0.135122059016776, ['envision', 'as', 'a', 'three-year', 'collaborative', 'program,', 'intelligent', 'cloud', 'hub', 'will', 'support', 'around', '100', 'institutions', 'with', 'ai', 'infrastructure,', 'course', 'content', 'and', 'curriculum,', 'developer', 'support,', 'development', 'tool', 'and', 'give', 'students', 'access', 'to', 'cloud', 'and', 'ai', 'service']), (0.12103286470786448, ['the', 'company', 'will', 'provide', 'ai', 'development', 'tool', 'and', 'azure', 'ai', 'service', 'such', 'as', 'microsoft', 'cognitive', 'services,', 'bot', 'service', 'and', 'azure', 'machine', 'learning.according', 'to', 'manish', 'prakash,', 'country', 'general', 'manager-ps,', 'health', 'and', 'education,', 'microsoft', 'india,', 'said,', 'with', 'ai', 'be', 'the', 'define', 'technology', 'of', 'our', 'time,', 'it', 'be', 'transform', 'live', 'and', 'industry', 'and', 'the', 'job', 'of', 'tomorrow', 'will', 'require', 'a', 'different', 'skillset']), (0.10446698067997025, ['the', 'program', 'aim', 'to', 'build', 'up', 'the', 'cognitive', 'skills', 'and', 'in-depth', 'understand', 'of', 'develop', 'intelligent', 'cloud', 'connect', 'solutions', 'for', 'applications', 'across', 'industry']), (0.1026400790554465, ['in', 'an', 'attempt', 'to', 'build', 'an', 'ai-ready', 'workforce,', 'microsoft', 'announce', 'intelligent', 'cloud', 'hub', 'which', 'have', 'be', 'launch', 'to', 'empower', 'the', 'next', 'generation', 'of', 'students', 'with', 'ai-ready', 'skills']), (0.1021710773289544, ['earlier', 'in', 'april', 'this', 'year,', 'the', 'company', 'announce', 'microsoft', 'professional', 'program', 'in', 'ai', 'as', 'a', 'learn', 'track', 'open', 'to', 'the', 'public']), (0.09992469144115354, ['as', 'part', 'of', 'the', 'program,', 'the', 'redmond', 'giant', 'which', 'want', 'to', 'expand', 'its', 'reach', 'and', 'be', 'plan', 'to', 'build', 'a', 'strong', 'developer', 'ecosystem', 'in', 'india', 'with', 'the', 'program', 'will', 'set', 'up', 'the', 'core', 'ai', 'infrastructure', 'and', 'iot', 'hub', 'for', 'the', 'select', 'campuses']), (0.09883464053277038, ['the', 'program', 'be', 'develop', 'to', 'provide', 'job', 'ready', 'skills', 'to', 'programmers', 'who', 'want', 'to', 'hone', 'their', 'skills', 'in', 'ai', 'and', 'data', 'science', 'with', 'a', 'series', 'of', 'online', 'course', 'which', 'feature', 'hands-on', 'labs', 'and', 'expert', 'instructors', 'as', 'well']), (0.08764062959672074, ['that’s', 'why', 'it', 'have', 'become', 'more', 'critical', 'than', 'ever', 'for', 'educational', 'institutions', 'to', 'integrate', 'new', 'cloud', 'and', 'ai', 'technologies']), (0.08481380033214689, ['this', 'will', 'require', 'more', 'collaborations', 'and', 'train', 'and', 'work', 'with', 'ai']), (0.06335317730819662, ['the', 'program', 'be', 'an', 'attempt', 'to', 'ramp', 'up', 'the', 'institutional', 'set-up', 'and', 'build', 'capabilities', 'among', 'the', 'educators', 'to', 'educate', 'the', 'workforce', 'of', 'tomorrow'])]\n",
            "Summarize Text: \n",
            " Envisioned as a three-year collaborative program, Intelligent Cloud Hub will support around 100 institutions with AI infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and AI services. The company will provide AI development tools and Azure AI services such as Microsoft Cognitive Services, Bot Services and Azure Machine Learning.According to Manish Prakash, Country General Manager-PS, Health and Education, Microsoft India, said, With AI being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset \n",
            "\n",
            "Summarize Text (with lower case): \n",
            " envisioned as a three-year collaborative program, intelligent cloud hub will support around 100 institutions with ai infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and ai services. the company will provide ai development tools and azure ai services such as microsoft cognitive services, bot services and azure machine learning.according to manish prakash, country general manager-ps, health and education, microsoft india, said, with ai being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset \n",
            "\n",
            "Summarize Text (with lower case & punctuation): \n",
            " envisioned as a three-year collaborative program, intelligent cloud hub will support around 100 institutions with ai infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and ai services. the company will provide ai development tools and azure ai services such as microsoft cognitive services, bot services and azure machine learning.according to manish prakash, country general manager-ps, health and education, microsoft india, said, with ai being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset \n",
            "\n",
            "Summarize Text (with lower case, punctuation & pronoun): \n",
            " envisioned as a three-year collaborative program, intelligent cloud hub will support around 100 institutions with ai infrastructure, course content and curriculum, developer support, development tools and give students access to cloud and ai services. the company will provide ai development tools and azure ai services such as microsoft cognitive services, bot services and azure machine learning.according to manish prakash, country general manager-ps, health and education, microsoft india, said, with ai being the defining technology of our time, it is transforming lives and industry and the jobs of tomorrow will require a different skillset \n",
            "\n",
            "Summarize Text (with lower case, punctuation, pronoun & lemmatization): \n",
            " envision as a three-year collaborative program, intelligent cloud hub will support around 100 institutions with ai infrastructure, course content and curriculum, developer support, development tool and give students access to cloud and ai service. the company will provide ai development tool and azure ai service such as microsoft cognitive services, bot service and azure machine learning.according to manish prakash, country general manager-ps, health and education, microsoft india, said, with ai be the define technology of our time, it be transform live and industry and the job of tomorrow will require a different skillset \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Comparison for different texts"
      ],
      "metadata": {
        "id": "h5C5xVcqEpXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary( \"trump.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9AHMf2dhz7LS",
        "outputId": "d46d5dfa-6aac-4e0b-e598-41281d557f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.21481805670368562, ['WASHINGTON', '-', 'The', 'Trump', 'administration', 'has', 'ordered', 'the', 'military', 'to', 'start', 'withdrawing', 'roughly', '7,000', 'troops', 'from', 'Afghanistan', 'in', 'the', 'coming', 'months,', 'two', 'defense', 'officials', 'said', 'Thursday,', 'an', 'abrupt', 'shift', 'in', 'the', '17-year-old', 'war', 'there', 'and', 'a', 'decision', 'that', 'stunned', 'Afghan', 'officials,', 'who', 'said', 'they', 'had', 'not', 'been', 'briefed', 'on', 'the', 'plans.President', 'Trump', 'made', 'the', 'decision', 'to', 'pull', 'the', 'troops', '-', 'about', 'half', 'the', 'number', 'the', 'United', 'States', 'has', 'in', 'Afghanistan', 'now', '-', 'at', 'the', 'same', 'time', 'he', 'decided', 'to', 'pull', 'American', 'forces', 'out', 'of', 'Syria,', 'one', 'official', 'said.The', 'announcement', 'came', 'hours', 'after', 'Jim', 'Mattis,', 'the', 'secretary', 'of', 'defense,', 'said', 'that', 'he', 'would', 'resign', 'from', 'his', 'position', 'at', 'the', 'end', 'of', 'February', 'after', 'disagreeing', 'with', 'the', 'president', 'over', 'his', 'approach', 'to', 'policy', 'in', 'the', 'Middle', 'East.The', 'whirlwind', 'of', 'troop', 'withdrawals', 'and', 'the', 'resignation', 'of', 'Mr']), (0.14235453910592732, ['Mattis,', 'he', 'begrudgingly', 'pledged', 'an', 'additional', '4,000', 'troops', 'to', 'the', 'Afghan', 'campaign', 'to', 'try', 'to', 'hasten', 'an', 'end', 'to', 'the', 'conflict.Though', 'Pentagon', 'officials', 'have', 'said', 'the', 'influx', 'of', 'forces', '-', 'coupled', 'with', 'a', 'more', 'aggressive', 'air', 'campaign', '-', 'was', 'helping', 'the', 'war', 'effort,', 'Afghan', 'forces', 'continued', 'to', 'take', 'nearly', 'unsustainable', 'levels', 'of', 'casualties', 'and', 'lose', 'ground', 'to', 'the', 'Taliban.The', 'renewed', 'American', 'effort', 'in', '2017', 'was', 'the', 'first', 'step', 'in', 'ensuring', 'Afghan', 'forces', 'could', 'become', 'more', 'independent', 'without', 'a', 'set', 'timeline', 'for', 'a', 'withdrawal']), (0.12147806249060758, ['Trump', 'might', 'take', 'impulsive', 'actions,', 'however,', 'often', 'loomed', 'in', 'the', 'background', 'of', 'discussions', 'with', 'the', 'United', 'States,', 'they', 'said.They', 'saw', 'the', 'abrupt', 'decision', 'as', 'a', 'further', 'sign', 'that', 'voices', 'from', 'the', 'ground', 'were', 'lacking', 'in', 'the', 'debate', 'over', 'the', 'war', 'and', 'that', 'with', 'Mr']), (0.11153559318610856, ['Several', 'Afghan', 'officials,', 'often', 'in', 'the', 'loop', 'on', 'security', 'planning', 'and', 'decision-making,', 'said', 'they', 'had', 'received', 'no', 'indication', 'in', 'recent', 'days', 'that', 'the', 'Americans', 'would', 'pull', 'troops', 'out']), (0.11140143490616569, ['Mattis’s', 'resignation,', 'Afghanistan', 'had', 'lost', 'one', 'of', 'the', 'last', 'influential', 'voices', 'in', 'Washington', 'who', 'channeled', 'the', 'reality', 'of', 'the', 'conflict', 'into', 'the', 'White', 'House’s', 'deliberations.The', 'president', 'long', 'campaigned', 'on', 'bringing', 'troops', 'home,', 'but', 'in', '2017,', 'at', 'the', 'request', 'of', 'Mr']), (0.08975918313916043, ['But', 'with', 'plans', 'to', 'quickly', 'reduce', 'the', 'number', 'of', 'American', 'troops', 'in', 'the', 'country,', 'it', 'is', 'unclear', 'if', 'the', 'Afghans', 'can', 'hold', 'their', 'own', 'against', 'an', 'increasingly', 'aggressive', 'Taliban.Currently,', 'American', 'airstrikes', 'are', 'at', 'levels', 'not', 'seen', 'since', 'the', 'height', 'of', 'the', 'war,', 'when', 'tens', 'of', 'thousands', 'of', 'American', 'troops', 'were', 'spread', 'throughout', 'the', 'country']), (0.0793514328956062, ['The', 'United', 'States', 'has', 'also', 'been', 'conducting', 'talks', 'with', 'representatives', 'of', 'the', 'Taliban,', 'in', 'what', 'officials', 'have', 'described', 'as', 'discussions', 'that', 'could', 'lead', 'to', 'formal', 'talks', 'to', 'end', 'the', 'conflict.Senior', 'Afghan', 'officials', 'and', 'Western', 'diplomats', 'in', 'Kabul', 'woke', 'up', 'to', 'the', 'shock', 'of', 'the', 'news', 'on', 'Friday', 'morning,', 'and', 'many', 'of', 'them', 'braced', 'for', 'chaos', 'ahead']), (0.0741717396389851, ['The', 'fear', 'that', 'Mr']), (0.05512995793375359, ['Mattis', 'leave', 'a', 'murky', 'picture', 'for', 'what', 'is', 'next', 'in', 'the', 'United', 'States’', 'longest', 'war,', 'and', 'they', 'come', 'as', 'Afghanistan', 'has', 'been', 'troubled', 'by', 'spasms', 'of', 'violence', 'afflicting', 'the', 'capital,', 'Kabul,', 'and', 'other', 'important', 'areas'])]\n",
            "Summarize Text: \n",
            " WASHINGTON - The Trump administration has ordered the military to start withdrawing roughly 7,000 troops from Afghanistan in the coming months, two defense officials said Thursday, an abrupt shift in the 17-year-old war there and a decision that stunned Afghan officials, who said they had not been briefed on the plans.President Trump made the decision to pull the troops - about half the number the United States has in Afghanistan now - at the same time he decided to pull American forces out of Syria, one official said.The announcement came hours after Jim Mattis, the secretary of defense, said that he would resign from his position at the end of February after disagreeing with the president over his approach to policy in the Middle East.The whirlwind of troop withdrawals and the resignation of Mr. Mattis, he begrudgingly pledged an additional 4,000 troops to the Afghan campaign to try to hasten an end to the conflict.Though Pentagon officials have said the influx of forces - coupled with a more aggressive air campaign - was helping the war effort, Afghan forces continued to take nearly unsustainable levels of casualties and lose ground to the Taliban.The renewed American effort in 2017 was the first step in ensuring Afghan forces could become more independent without a set timeline for a withdrawal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary( \"trump.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h1MdjIek0JZ7",
        "outputId": "22e31794-6ec4-43e0-c561-e6e8ba9c851a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.17825073056841495, ['washington', '-', 'the', 'trump', 'administration', 'have', 'order', 'the', 'military', 'to', 'start', 'withdraw', 'roughly', '7000', 'troop', 'from', 'afghanistan', 'in', 'the', 'come', 'months', 'two', 'defense', 'officials', 'say', 'thursday', 'an', 'abrupt', 'shift', 'in', 'the', '17-year-old', 'war', 'there', 'and', 'a', 'decision', 'that', 'stun', 'afghan', 'officials', 'who', 'say', 'they', 'have', 'not', 'be', 'brief', 'on', 'the', 'planspresident', 'trump', 'make', 'the', 'decision', 'to', 'pull', 'the', 'troop', '-', 'about', 'half', 'the', 'number', 'the', 'unite', 'state', 'have', 'in', 'afghanistan', 'now', '-', 'at', 'the', 'same', 'time', 'he', 'decide', 'to', 'pull', 'american', 'force', 'out', 'of', 'syria', 'one', 'official', 'saidthe', 'announcement', 'come', 'hours', 'after', 'jim', 'mattis', 'the', 'secretary', 'of', 'defense', 'say', 'that', 'he', 'would', 'resign', 'from', 'his', 'position', 'at', 'the', 'end', 'of', 'february', 'after', 'disagree', 'with', 'the', 'president', 'over', 'his', 'approach', 'to', 'policy', 'in', 'the', 'middle', 'eastthe', 'whirlwind', 'of', 'troop', 'withdrawals', 'and', 'the', 'resignation', 'of', 'mr']), (0.1319297588101058, ['mattis', 'he', 'begrudgingly', 'pledge', 'an', 'additional', '4000', 'troop', 'to', 'the', 'afghan', 'campaign', 'to', 'try', 'to', 'hasten', 'an', 'end', 'to', 'the', 'conflictthough', 'pentagon', 'officials', 'have', 'say', 'the', 'influx', 'of', 'force', '-', 'couple', 'with', 'a', 'more', 'aggressive', 'air', 'campaign', '-', 'be', 'help', 'the', 'war', 'effort', 'afghan', 'force', 'continue', 'to', 'take', 'nearly', 'unsustainable', 'level', 'of', 'casualties', 'and', 'lose', 'grind', 'to', 'the', 'talibanthe', 'renew', 'american', 'effort', 'in', '2017', 'be', 'the', 'first', 'step', 'in', 'ensure', 'afghan', 'force', 'could', 'become', 'more', 'independent', 'without', 'a', 'set', 'timeline', 'for', 'a', 'withdrawal']), (0.1175277459961717, ['that', 'air', 'support', 'officials', 'say', 'consist', 'mostly', 'of', 'prop', 'up', 'afghan', 'troop', 'while', 'they', 'try', 'to', 'hold', 'territory', 'from', 'a', 'resurgent', 'taliban']), (0.11228016411140762, ['several', 'afghan', 'officials', 'often', 'in', 'the', 'loop', 'on', 'security', 'plan', 'and', 'decision-making', 'say', 'they', 'have', 'receive', 'no', 'indication', 'in', 'recent', 'days', 'that', 'the', 'americans', 'would', 'pull', 'troop', 'out']), (0.09228781377514532, ['trump', 'might', 'take', 'impulsive', 'action', 'however', 'often', 'loom', 'in', 'the', 'background', 'of', 'discussions', 'with', 'the', 'unite', 'state', 'they', 'saidthey', 'saw', 'the', 'abrupt', 'decision', 'as', 'a', 'further', 'sign', 'that', 'voice', 'from', 'the', 'grind', 'be', 'lack', 'in', 'the', 'debate', 'over', 'the', 'war', 'and', 'that', 'with', 'mr']), (0.09006775236163013, ['mattis’s', 'resignation', 'afghanistan', 'have', 'lose', 'one', 'of', 'the', 'last', 'influential', 'voice', 'in', 'washington', 'who', 'channel', 'the', 'reality', 'of', 'the', 'conflict', 'into', 'the', 'white', 'house’s', 'deliberationsthe', 'president', 'long', 'campaign', 'on', 'bring', 'troop', 'home', 'but', 'in', '2017', 'at', 'the', 'request', 'of', 'mr']), (0.08658582462696257, ['the', 'unite', 'state', 'have', 'also', 'be', 'conduct', 'talk', 'with', 'representatives', 'of', 'the', 'taliban', 'in', 'what', 'officials', 'have', 'describe', 'as', 'discussions', 'that', 'could', 'lead', 'to', 'formal', 'talk', 'to', 'end', 'the', 'conflictsenior', 'afghan', 'officials', 'and', 'western', 'diplomats', 'in', 'kabul', 'wake', 'up', 'to', 'the', 'shock', 'of', 'the', 'news', 'on', 'friday', 'morning', 'and', 'many', 'of', 'them', 'brace', 'for', 'chaos', 'ahead']), (0.08141487637533773, ['but', 'with', 'plan', 'to', 'quickly', 'reduce', 'the', 'number', 'of', 'american', 'troop', 'in', 'the', 'country', 'it', 'be', 'unclear', 'if', 'the', 'afghans', 'can', 'hold', 'their', 'own', 'against', 'an', 'increasingly', 'aggressive', 'talibancurrently', 'american', 'airstrikes', 'be', 'at', 'level', 'not', 'see', 'since', 'the', 'height', 'of', 'the', 'war', 'when', 'tens', 'of', 'thousands', 'of', 'american', 'troop', 'be', 'spread', 'throughout', 'the', 'country']), (0.05922755984420541, ['mattis', 'leave', 'a', 'murky', 'picture', 'for', 'what', 'be', 'next', 'in', 'the', 'unite', 'states’', 'longest', 'war', 'and', 'they', 'come', 'as', 'afghanistan', 'have', 'be', 'trouble', 'by', 'spasms', 'of', 'violence', 'afflict', 'the', 'capital', 'kabul', 'and', 'other', 'important', 'areas']), (0.05042777353061867, ['the', 'fear', 'that', 'mr'])]\n",
            "Summarize Text: \n",
            " washington - the trump administration have order the military to start withdraw roughly 7000 troop from afghanistan in the come months two defense officials say thursday an abrupt shift in the 17-year-old war there and a decision that stun afghan officials who say they have not be brief on the planspresident trump make the decision to pull the troop - about half the number the unite state have in afghanistan now - at the same time he decide to pull american force out of syria one official saidthe announcement come hours after jim mattis the secretary of defense say that he would resign from his position at the end of february after disagree with the president over his approach to policy in the middle eastthe whirlwind of troop withdrawals and the resignation of mr. mattis he begrudgingly pledge an additional 4000 troop to the afghan campaign to try to hasten an end to the conflictthough pentagon officials have say the influx of force - couple with a more aggressive air campaign - be help the war effort afghan force continue to take nearly unsustainable level of casualties and lose grind to the talibanthe renew american effort in 2017 be the first step in ensure afghan force could become more independent without a set timeline for a withdrawal\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary( \"fb.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-h6dlygz656",
        "outputId": "6960aa23-1d25-489e-8a5c-e404d8c130bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.25755053984117615, ['For', 'years,', 'Facebook', 'gave', 'some', 'of', 'the', \"world's\", 'largest', 'technology', 'companies', 'more', 'intrusive', 'access', 'to', \"users'\", 'personal', 'data', 'than', 'it', 'has', 'disclosed,', 'effectively', 'exempting', 'those', 'business', 'partners', 'from', 'its', 'usual', 'privacy', 'rules,', 'according', 'to', 'internal', 'records', 'and', 'interviews']), (0.15321342634501522, ['Facebook', 'users', 'connected', 'with', 'friends', 'across', 'different', 'devices', 'and', 'websites']), (0.1484419990863834, ['Pushing', 'for', 'explosive', 'growth,', 'Facebook', 'got', 'more', 'users,', 'lifting', 'its', 'advertising', 'revenue']), (0.14021994428057658, ['The', 'special', 'arrangements', 'are', 'detailed', 'in', 'hundreds', 'of', 'pages', 'of', 'Facebook', 'documents', 'obtained', 'by', 'The', 'New', 'York', 'Times']), (0.1351454272881601, ['They', 'also', 'underscore', 'how', 'personal', 'data', 'has', 'become', 'the', 'most', 'prized', 'commodity', 'of', 'the', 'digital', 'age,', 'traded', 'on', 'a', 'vast', 'scale', 'by', 'some', 'of', 'the', 'most', 'powerful', 'companies', 'in', 'Silicon', 'Valley', 'and', 'beyond']), (0.10012172020682458, ['Partner', 'companies', 'acquired', 'features', 'to', 'make', 'their', 'products', 'more', 'attractive']), (0.04432792197284274, ['The', 'records,', 'generated', 'in', '2017', 'by', 'the', \"company's\", 'internal', 'system', 'for', 'tracking', 'partnerships,', 'provide', 'the', 'most', 'complete', 'picture', 'yet', 'of', 'the', 'social', \"network's\", 'data-sharing', 'practices']), (0.020979020979020983, ['The', 'exchange', 'was', 'intended', 'to', 'benefit', 'everyone'])]\n",
            "Summarize Text: \n",
            " For years, Facebook gave some of the world's largest technology companies more intrusive access to users' personal data than it has disclosed, effectively exempting those business partners from its usual privacy rules, according to internal records and interviews. Facebook users connected with friends across different devices and websites\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary( \"fb.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mT9hzsc10Ld8",
        "outputId": "f7fb89d3-ac8c-4473-f8b4-f8ee0b82bc9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.1863994516865572, ['for', 'years', 'facebook', 'give', 'some', 'of', 'the', 'world', 'largest', 'technology', 'company', 'more', 'intrusive', 'access', 'to', \"users'\", 'personal', 'data', 'than', 'it', 'have', 'disclose', 'effectively', 'exempt', 'those', 'business', 'partner', 'from', 'its', 'usual', 'privacy', 'rule', 'accord', 'to', 'internal', 'record', 'and', 'interview']), (0.18300916139630743, ['but', 'facebook', 'also', 'assume', 'extraordinary', 'power', 'over', 'the', 'personal', 'information', 'of', 'its', '2', 'billion', 'users', '-', 'control', 'it', 'have', 'wield', 'with', 'little', 'transparency', 'or', 'outside', 'oversightfacebook', 'allow', 'microsoft', 'bing', 'search', 'engine', 'to', 'see', 'the', 'name', 'of', 'virtually', 'all', 'facebook', 'user', 'friends', 'without', 'consent', 'the', 'record', 'show', 'and', 'give', 'netflix', 'and', 'spotify', 'the', 'ability', 'to', 'read', 'facebook', \"users'\", 'private', 'message']), (0.137935602276032, ['facebook', 'users', 'connect', 'with', 'friends', 'across', 'different', 'devices', 'and', 'websites']), (0.12451608065151104, ['push', 'for', 'explosive', 'growth', 'facebook', 'get', 'more', 'users', 'lift', 'its', 'advertise', 'revenue']), (0.09011996163143693, ['they', 'also', 'underscore', 'how', 'personal', 'data', 'have', 'become', 'the', 'most', 'prize', 'commodity', 'of', 'the', 'digital', 'age', 'trade', 'on', 'a', 'vast', 'scale', 'by', 'some', 'of', 'the', 'most', 'powerful', 'company', 'in', 'silicon', 'valley', 'and', 'beyond']), (0.08882604801444643, ['the', 'special', 'arrangements', 'be', 'detail', 'in', 'hundreds', 'of', 'page', 'of', 'facebook', 'document', 'obtain', 'by', 'the', 'new', 'york', 'time']), (0.08542253721789658, ['partner', 'company', 'acquire', 'feature', 'to', 'make', 'their', 'products', 'more', 'attractive']), (0.08536624915035215, ['the', 'record', 'generate', 'in', '2017', 'by', 'the', 'company', 'internal', 'system', 'for', 'track', 'partnerships', 'provide', 'the', 'most', 'complete', 'picture', 'yet', 'of', 'the', 'social', 'network', 'data-sharing', 'practice']), (0.01840490797546013, ['the', 'exchange', 'be', 'intend', 'to', 'benefit', 'everyone'])]\n",
            "Summarize Text: \n",
            " for years facebook give some of the world largest technology company more intrusive access to users' personal data than it have disclose effectively exempt those business partner from its usual privacy rule accord to internal record and interview. but facebook also assume extraordinary power over the personal information of its 2 billion users - control it have wield with little transparency or outside oversightfacebook allow microsoft bing search engine to see the name of virtually all facebook user friends without consent the record show and give netflix and spotify the ability to read facebook users' private message\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_summary( \"apple_mwc.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-i0yeJ9Zz6Yt",
        "outputId": "a5624c6e-2058-4aca-8216-98c75c09343c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.09501693609795944, ['While', 'rumors', 'point', 'to', 'an', 'imminent', 'announcement,', \"it's\", 'unclear', 'how', 'much', 'Apple', 'might', 'charge', 'for', 'the', 'new', 'Air', 'or', 'how', 'it', 'might', 'fit', 'into', 'the', \"company's\", 'existing', 'MacBook', 'lineup']), (0.07572644199422097, ['Apple', 'last', 'updated', 'the', 'Mac', 'Pro', 'at', 'WWDC', 'in', '2019']), (0.06673847113933327, ['The', '2020', 'M1-powered', '13.3-inch', 'MacBook', 'Air', 'is', 'still', 'sold', 'for', '999,', 'while', 'the', 'updated', 'M2-powered', '2022', '13.6-inch', 'MacBook', 'Air', 'starts', 'at', '1,199']), (0.06587142183034832, ['In', 'an', 'April', 'appearance', 'on', 'The', 'MacRumors', 'Show,', 'Gurman,', 'the', 'Apple', 'savant,', 'suggests', 'that', 'it', 'still', 'may', 'arrive', 'this', 'year', 'but', 'not', 'at', 'WWDC']), (0.06583552689160813, ['Might', 'the', '15-inch', 'Air', 'fit', 'somewhere', 'in', 'the', 'middle?', 'Oh,', 'the', 'Mac', 'Pro']), (0.06273008531708853, [\"Bloomberg's\", 'Mark', 'Gurman', 'recently', 'detailed', 'how', 'Apple', 'plans', 'to', 'incorporate', 'sports,', 'gaming,', 'workouts', 'and', 'iPad', 'apps', 'into', 'the', 'headset', 'to', 'show', 'off', 'what', 'the', 'new', 'platform', 'can', 'do']), (0.06025938565923405, ['The', 'biggest', 'rumor', 'heading', 'into', 'this', \"year's\", 'WWDC', 'is', 'of', 'course', 'the', 'Apple', 'headset']), (0.055373618517471516, ['Despite', 'some', 'teases', 'that', 'confirmed', \"it's\", 'working', 'on', 'a', 'new', 'one', 'powered', 'by', 'its', 'Apple', 'Silicon', 'chips,', 'the', 'company', 'has', 'largely', 'been', 'quiet', 'about', 'the', 'super', 'powerful', 'computer']), (0.05477140188752459, ['The', 'MacBook', 'Air', 'has', 'long', 'been', 'one', 'of', \"Apple's\", 'most', 'popular', 'laptops']), (0.05414793791119278, ['Among', 'the', 'bigger', 'iOS', 'changes,', 'Apple', 'might', 'finally', 'add', 'support', 'for', 'installing', 'apps', 'not', 'downloaded', 'from', 'the', 'App', 'Store']), (0.045497876048900154, ['Might', 'the', 'another', 'day', 'be', 'June', '5?', \"It's\", 'possible', 'and', 'Mac', 'Pro', 'fans', 'may', 'want', 'to', 'tune', 'in,', 'but', 'with', 'tempered', 'expectations']), (0.044334057237670366, ['The', 'iPhone', 'maker', 'has', 'long', 'resisted', 'opening', 'up', 'its', 'mobile', 'software', 'to', 'allow', 'for', 'sideloading,', 'but', 'new', 'European', 'regulations', 'may', 'have', 'forced', 'its', 'hand']), (0.03966331247616724, ['That', 'once', 'again', 'comes', 'from', \"Bloomberg's\", 'Gurman,', 'who', 'expects', 'the', 'new', 'laptop', 'to', 'be', 'announced', 'at', 'this', \"year's\", 'event']), (0.037677594001304666, ['Rumors', 'these', 'days,', 'however,', 'suggest', 'that', 'the', 'company', 'has', 'a', 'larger,', '15-inch', 'M2-powered', 'Air', 'raring', 'to', 'go']), (0.035811867116440826, ['In', 'addition', 'to', 'all', 'the', 'hardware', 'rumors,', 'we', 'can', 'expect', 'Apple', 'to', 'detail', 'the', 'latest', 'updates', 'coming', 'this', 'year', 'to', 'its', 'iOS,', 'iPadOS,', 'WatchOS', 'and', 'TVOS', 'platforms']), (0.03510290972867689, ['Frequently', 'sold', 'with', 'a', '13-inch', 'screen,', 'Apple', 'has', 'experimented', 'with', 'different', 'sizes', 'of', 'Airs', 'in', 'the', 'past,', 'including', 'offering', 'an', '11-inch', 'model', 'for', 'years']), (0.029054712711663475, ['A', '16-inch', 'MacBook', 'Pro,', 'meanwhile,', 'starts', 'at', '2,499']), (0.02579395939445373, ['There', 'may', 'be', 'eye', 'and', 'hand', 'tracking,', 'high-resolution', 'displays', 'and..']), (0.02243114524285989, ['Rumored', 'to', 'be', 'running', 'on', 'a', 'new', 'xrOS,', 'the', 'device', 'could', 'utilize', 'mixed', 'reality,', 'a', 'combination', 'of', 'virtual', 'reality', 'and', 'augmented', 'reality']), (0.013383013672728289, ['Whether', \"that's\", 'enough', 'to', 'excite', 'consumers', 'and', 'persuade', 'them', 'to', 'drop', 'three', 'grand', 'or', 'for', 'developers', 'to', 'commit', 'to', 'building', 'apps', 'for', 'it', 'remains', 'to', 'be', 'seen']), (0.0073891625615763665, ['a', 'potential', '3,000', 'price', 'tag']), (0.0073891625615763665, [\"It's\", 'about', 'time'])]\n",
            "Summarize Text: \n",
            " While rumors point to an imminent announcement, it's unclear how much Apple might charge for the new Air or how it might fit into the company's existing MacBook lineup. Apple last updated the Mac Pro at WWDC in 2019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary( \"apple_mwc.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIiG64QZ0Odh",
        "outputId": "cabfd525-1ba8-4ed1-847f-673fcbf4077f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.0802774753894062, ['while', 'rumor', 'point', 'to', 'an', 'imminent', 'announcement', 'it', 'unclear', 'how', 'much', 'apple', 'might', 'charge', 'for', 'the', 'new', 'air', 'or', 'how', 'it', 'might', 'fit', 'into', 'the', 'company', 'exist', 'macbook', 'lineup']), (0.07009732384838405, ['other', 'software', 'change', 'apple', 'might', 'unveil', 'include', 'a', 'new', 'mental', 'health', 'app', 'as', 'well', 'as', 'widgets', 'return', 'to', 'the', 'apple', 'watch']), (0.06885837412975318, ['the', 'macbook', 'air', 'have', 'long', 'be', 'one', 'of', 'apple', 'most', 'popular', 'laptops']), (0.06435356252397864, ['apple', 'last', 'update', 'the', 'mac', 'pro', 'at', 'wwdc', 'in', '2019']), (0.06128384725192808, ['the', 'biggest', 'rumor', 'head', 'into', 'this', 'year', 'wwdc', 'be', 'of', 'course', 'the', 'apple', 'headset']), (0.06047299980677545, ['in', 'an', 'april', 'appearance', 'on', 'the', 'macrumors', 'show', 'gurman', 'the', 'apple', 'savant', 'suggest', 'that', 'it', 'still', 'may', 'arrive', 'this', 'year', 'but', 'not', 'at', 'wwdc']), (0.055245818816752376, ['bloomberg', 'mark', 'gurman', 'recently', 'detail', 'how', 'apple', 'plan', 'to', 'incorporate', 'sport', 'game', 'workouts', 'and', 'ipad', 'apps', 'into', 'the', 'headset', 'to', 'show', 'off', 'what', 'the', 'new', 'platform', 'can', 'do']), (0.054789694238504855, ['in', 'addition', 'to', 'all', 'the', 'hardware', 'rumor', 'we', 'can', 'expect', 'apple', 'to', 'detail', 'the', 'latest', 'update', 'come', 'this', 'year', 'to', 'its', 'ios', 'ipados', 'watchos', 'and', 'tvos', 'platforms']), (0.052106539250537404, ['might', 'the', '15-inch', 'air', 'fit', 'somewhere', 'in', 'the', 'middle', 'oh', 'the', 'mac', 'pro']), (0.04927464117319323, ['among', 'the', 'bigger', 'ios', 'change', 'apple', 'might', 'finally', 'add', 'support', 'for', 'instal', 'apps', 'not', 'download', 'from', 'the', 'app', 'store']), (0.04777213152554727, ['the', '2020', 'm1-powered', '133-inch', 'macbook', 'air', 'be', 'still', 'sell', 'for', '999', 'while', 'the', 'update', 'm2-powered', '2022', '136-inch', 'macbook', 'air', 'start', 'at', '1199']), (0.04399669128200352, ['despite', 'some', 'tease', 'that', 'confirm', 'it', 'work', 'on', 'a', 'new', 'one', 'power', 'by', 'its', 'apple', 'silicon', 'chip', 'the', 'company', 'have', 'largely', 'be', 'quiet', 'about', 'the', 'super', 'powerful', 'computer']), (0.04234730130804794, ['frequently', 'sell', 'with', 'a', '13-inch', 'screen', 'apple', 'have', 'experiment', 'with', 'different', 'size', 'of', 'air', 'in', 'the', 'past', 'include', 'offer', 'an', '11-inch', 'model', 'for', 'years']), (0.03963962268435432, ['that', 'once', 'again', 'come', 'from', 'bloomberg', 'gurman', 'who', 'expect', 'the', 'new', 'laptop', 'to', 'be', 'announce', 'at', 'this', 'year', 'event']), (0.03887492644236196, ['rumor', 'these', 'days', 'however', 'suggest', 'that', 'the', 'company', 'have', 'a', 'larger', '15-inch', 'm2-powered', 'air', 'raring', 'to', 'go']), (0.03738422927954525, ['might', 'the', 'another', 'day', 'be', 'june', '5', 'it', 'possible', 'and', 'mac', 'pro', 'fan', 'may', 'want', 'to', 'tune', 'in', 'but', 'with', 'temper', 'expectations']), (0.03404883355729226, ['the', 'iphone', 'maker', 'have', 'long', 'resist', 'open', 'up', 'its', 'mobile', 'software', 'to', 'allow', 'for', 'sideloading', 'but', 'new', 'european', 'regulations', 'may', 'have', 'force', 'its', 'hand']), (0.031099753889305946, ['a', '16-inch', 'macbook', 'pro', 'meanwhile', 'start', 'at', '2499']), (0.02324510141449977, ['rumor', 'to', 'be', 'run', 'on', 'a', 'new', 'xros', 'the', 'device', 'could', 'utilize', 'mix', 'reality', 'a', 'combination', 'of', 'virtual', 'reality', 'and', 'augment', 'reality']), (0.0199456656684852, ['there', 'may', 'be', 'eye', 'and', 'hand', 'track', 'high-resolution', 'display', 'and']), (0.010800959476735088, ['whether', 'that', 'enough', 'to', 'excite', 'consumers', 'and', 'persuade', 'them', 'to', 'drop', 'three', 'grand', 'or', 'for', 'developers', 'to', 'commit', 'to', 'build', 'apps', 'for', 'it', 'remain', 'to', 'be', 'see']), (0.007042253521304075, ['it', 'about', 'time']), (0.007042253521304075, ['a', 'potential', '3000', 'price', 'tag'])]\n",
            "Summarize Text: \n",
            " while rumor point to an imminent announcement it unclear how much apple might charge for the new air or how it might fit into the company exist macbook lineup. other software change apple might unveil include a new mental health app as well as widgets return to the apple watch\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Can summarize paragraphs as well (from ICE-1)"
      ],
      "metadata": {
        "id": "76qZrudB1xYM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/miguelfzafra/Latest-News-Classifier/master/0.%20Latest%20News%20Classifier/00.%20Raw%20dataset/BBC/bbc-fulltext/bbc/business/001.txt')\n",
        "data = response.text\n",
        "print(data,  file=open('business_001.txt', 'w'))\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/miguelfzafra/Latest-News-Classifier/master/0.%20Latest%20News%20Classifier/00.%20Raw%20dataset/BBC/bbc-fulltext/bbc/business/002.txt')\n",
        "data = response.text\n",
        "print(data,  file=open('business_002.txt', 'w'))\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/miguelfzafra/Latest-News-Classifier/master/0.%20Latest%20News%20Classifier/00.%20Raw%20dataset/BBC/bbc-fulltext/bbc/business/003.txt')\n",
        "data = response.text\n",
        "print(data,  file=open('business_003.txt', 'w'))\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/miguelfzafra/Latest-News-Classifier/master/0.%20Latest%20News%20Classifier/00.%20Raw%20dataset/BBC/bbc-fulltext/bbc/business/004.txt')\n",
        "data = response.text\n",
        "print(data,  file=open('business_004.txt', 'w'))\n",
        "\n",
        "response = requests.get('https://raw.githubusercontent.com/miguelfzafra/Latest-News-Classifier/master/0.%20Latest%20News%20Classifier/00.%20Raw%20dataset/BBC/bbc-fulltext/bbc/business/005.txt')\n",
        "data = response.text\n",
        "print(data,  file=open('business_005.txt', 'w'))"
      ],
      "metadata": {
        "id": "eE2F8yhs2fdm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary_blog(\"business_001.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ij5wq83myYUk",
        "outputId": "4a679038-3c1c-4212-cafd-42e6e6f7e51d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.21293950793362676, ['the', 'firm', 'which', 'be', 'now', 'one', 'of', 'the', 'biggest', 'investors', 'in', 'google', 'benefit', 'from', 'sales', 'of', 'high-speed', 'internet', 'connections', 'and', 'higher', 'advert', 'sales', 'timewarner', 'say', 'fourth', 'quarter', 'sales', 'rise', '2%', 'to', '$111bn', 'from', '$109bn', 'its', 'profit', 'be', 'buoy', 'by', 'one-off', 'gain', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'warner', 'bros', 'and', 'less', 'users', 'for', 'aol']), (0.2091791279867516, ['time', 'warner', 'say', 'on', 'friday', 'that', 'it', 'now', 'own', '8%', 'of', 'search-engine', 'google', 'but', 'its', 'own', 'internet', 'business', 'aol', 'have', 'have', 'mix', 'fortunes', 'it', 'lose', '464000', 'subscribers', 'in', 'the', 'fourth', 'quarter', 'profit', 'be', 'lower', 'than', 'in', 'the', 'precede', 'three', 'quarter', 'however', 'the', 'company', 'say', 'aol', 'underlie', 'profit', 'before', 'exceptional', 'items', 'rise', '8%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertise', 'revenues', 'it', 'hop', 'to', 'increase', 'subscribers', 'by', 'offer', 'the', 'online', 'service', 'free', 'to', 'timewarner', 'internet', 'customers', 'and', 'will', 'try', 'to', 'sign', 'up', 'aol', 'exist', 'customers', 'for', 'high-speed', 'broadband', 'timewarner', 'also', 'have', 'to', 'restate', '2000', 'and', '2003', 'result', 'follow', 'a', 'probe', 'by', 'the', 'us', 'securities', 'exchange', 'commission', '(sec)', 'which', 'be', 'close', 'to', 'conclude']), (0.18594225357018157, ['time', 'warner', 'fourth', 'quarter', 'profit', 'be', 'slightly', 'better', 'than', \"analysts'\", 'expectations', 'but', 'its', 'film', 'division', 'saw', 'profit', 'slump', '27%', 'to', '$284m', 'help', 'by', 'box-office', 'flop', 'alexander', 'and', 'catwoman', 'a', 'sharp', 'contrast', 'to', 'year-earlier', 'when', 'the', 'third', 'and', 'final', 'film', 'in', 'the', 'lord', 'of', 'the', 'ring', 'trilogy', 'boost', 'result', 'for', 'the', 'full-year', 'timewarner', 'post', 'a', 'profit', 'of', '$336bn', 'up', '27%', 'from', 'its', '2003', 'performance', 'while', 'revenues', 'grow', '64%', 'to', '$4209bn', '\"our', 'financial', 'performance', 'be', 'strong', 'meet', 'or', 'exceed', 'all', 'of', 'our', 'full-year', 'objectives', 'and', 'greatly', 'enhance', 'our', 'flexibility\"', 'chairman', 'and', 'chief', 'executive', 'richard', 'parsons', 'say', 'for', '2005', 'timewarner', 'be', 'project', 'operate', 'earn', 'growth', 'of', 'around', '5%', 'and', 'also', 'expect', 'higher', 'revenue', 'and', 'wider', 'profit', 'margins']), (0.17551083773768114, ['ad', 'sales', 'boost', 'time', 'warner', 'profit']), (0.1263663228982152, ['quarterly', 'profit', 'at', 'us', 'media', 'giant', 'timewarner', 'jump', '76%', 'to', '$113bn', '(£600m)', 'for', 'the', 'three', 'months', 'to', 'december', 'from', '$639m', 'year-earlier']), (0.09006194987354366, ['timewarner', 'be', 'to', 'restate', 'its', 'account', 'as', 'part', 'of', 'efforts', 'to', 'resolve', 'an', 'inquiry', 'into', 'aol', 'by', 'us', 'market', 'regulators', 'it', 'have', 'already', 'offer', 'to', 'pay', '$300m', 'to', 'settle', 'charge', 'in', 'a', 'deal', 'that', 'be', 'under', 'review', 'by', 'the', 'sec', 'the', 'company', 'say', 'it', 'be', 'unable', 'to', 'estimate', 'the', 'amount', 'it', 'need', 'to', 'set', 'aside', 'for', 'legal', 'reserve', 'which', 'it', 'previously', 'set', 'at', '$500m', 'it', 'intend', 'to', 'adjust', 'the', 'way', 'it', 'account', 'for', 'a', 'deal', 'with', 'german', 'music', 'publisher', 'bertelsmann', 'purchase', 'of', 'a', 'stake', 'in', 'aol', 'europe', 'which', 'it', 'have', 'report', 'as', 'advertise', 'revenue', 'it', 'will', 'now', 'book', 'the', 'sale', 'of', 'its', 'stake', 'in', 'aol', 'europe', 'as', 'a', 'loss', 'on', 'the', 'value', 'of', 'that', 'stake'])]\n",
            "Summarize Text: \n",
            " the firm which be now one of the biggest investors in google benefit from sales of high-speed internet connections and higher advert sales timewarner say fourth quarter sales rise 2% to $111bn from $109bn its profit be buoy by one-off gain which offset a profit dip at warner bros and less users for aol. time warner say on friday that it now own 8% of search-engine google but its own internet business aol have have mix fortunes it lose 464000 subscribers in the fourth quarter profit be lower than in the precede three quarter however the company say aol underlie profit before exceptional items rise 8% on the back of stronger internet advertise revenues it hop to increase subscribers by offer the online service free to timewarner internet customers and will try to sign up aol exist customers for high-speed broadband timewarner also have to restate 2000 and 2003 result follow a probe by the us securities exchange commission (sec) which be close to conclude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary_blog(\"business_002.txt\", 2)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BNNemwVG2iSn",
        "outputId": "f930dddc-3d96-4650-edbb-fc12085059b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.32384532222678786, ['and', 'alan', 'greenspan', 'highlight', 'the', 'us', 'government', 'willingness', 'to', 'curb', 'spend', 'and', 'rise', 'household', 'save', 'as', 'factor', 'which', 'may', 'help', 'to', 'reduce', 'it', 'in', 'late', 'trade', 'in', 'new', 'york', 'the', 'dollar', 'reach', '$12871', 'against', 'the', 'euro', 'from', '$12974', 'on', 'thursday', 'market', 'concern', 'about', 'the', 'deficit', 'have', 'hit', 'the', 'greenback', 'in', 'recent', 'months', 'on', 'friday', 'federal', 'reserve', 'chairman', 'mr', 'greenspan', 'speech', 'in', 'london', 'ahead', 'of', 'the', 'meet', 'of', 'g7', 'finance', 'minister', 'send', 'the', 'dollar', 'higher', 'after', 'it', 'have', 'earlier', 'tumble', 'on', 'the', 'back', 'of', 'worse-than-expected', 'us', 'job', 'data', '\"i', 'think', 'the', 'chairman', 'take', 'a', 'much', 'more', 'sanguine', 'view', 'on', 'the', 'current', 'account', 'deficit', 'than', 'he', 'take', 'for', 'some', 'time\"', 'say', 'robert', 'sinche', 'head', 'of', 'currency', 'strategy', 'at', 'bank', 'of', 'america', 'in', 'new', 'york', '\"he', 'take', 'a', 'longer-term', 'view', 'lay', 'out', 'a', 'set', 'of', 'condition', 'under', 'which', 'the', 'current', 'account', 'deficit', 'can', 'improve', 'this', 'year', 'and', 'next\"']), (0.27629800104417623, ['the', 'dollar', 'have', 'hit', 'its', 'highest', 'level', 'against', 'the', 'euro', 'in', 'almost', 'three', 'months', 'after', 'the', 'federal', 'reserve', 'head', 'say', 'the', 'us', 'trade', 'deficit', 'be', 'set', 'to', 'stabilise']), (0.22152488630557654, ['worry', 'about', 'the', 'deficit', 'concern', 'about', 'china', 'do', 'however', 'remain', 'china', 'currency', 'remain', 'peg', 'to', 'the', 'dollar', 'and', 'the', 'us', 'currency', 'sharp', 'fall', 'in', 'recent', 'months', 'have', 'therefore', 'make', 'chinese', 'export', 'price', 'highly', 'competitive', 'but', 'call', 'for', 'a', 'shift', 'in', 'beijing', 'policy', 'have', 'fall', 'on', 'deaf', 'ears', 'despite', 'recent', 'comment', 'in', 'a', 'major', 'chinese', 'newspaper', 'that', 'the', '\"time', 'be', 'ripe\"', 'for', 'a', 'loosen', 'of', 'the', 'peg', 'the', 'g7', 'meet', 'be', 'think', 'unlikely', 'to', 'produce', 'any', 'meaningful', 'movement', 'in', 'chinese', 'policy', 'in', 'the', 'meantime', 'the', 'us', 'federal', 'reserve', 'decision', 'on', '2', 'february', 'to', 'boost', 'interest', 'rat', 'by', 'a', 'quarter', 'of', 'a', 'point', '-', 'the', 'sixth', 'such', 'move', 'in', 'as', 'many', 'months', '-', 'have', 'open', 'up', 'a', 'differential', 'with', 'european', 'rat', 'the', 'half-point', 'window', 'some', 'believe', 'could', 'be', 'enough', 'to', 'keep', 'us', 'assets', 'look', 'more', 'attractive', 'and', 'could', 'help', 'prop', 'up', 'the', 'dollar', 'the', 'recent', 'fall', 'have', 'partly', 'be', 'the', 'result', 'of', 'big', 'budget', 'deficits', 'as', 'well', 'as', 'the', 'us', 'yawn', 'current', 'account', 'gap', 'both', 'of', 'which', 'need', 'to', 'be', 'fund', 'by', 'the', 'buy', 'of', 'us', 'bond', 'and', 'assets', 'by', 'foreign', 'firm', 'and', 'governments', 'the', 'white', 'house', 'will', 'announce', 'its', 'budget', 'on', 'monday', 'and', 'many', 'commentators', 'believe', 'the', 'deficit', 'will', 'remain', 'at', 'close', 'to', 'half', 'a', 'trillion', 'dollars']), (0.17833179042345965, ['dollar', 'gain', 'on', 'greenspan', 'speech'])]\n",
            "Summarize Text: \n",
            " and alan greenspan highlight the us government willingness to curb spend and rise household save as factor which may help to reduce it in late trade in new york the dollar reach $12871 against the euro from $12974 on thursday market concern about the deficit have hit the greenback in recent months on friday federal reserve chairman mr greenspan speech in london ahead of the meet of g7 finance minister send the dollar higher after it have earlier tumble on the back of worse-than-expected us job data \"i think the chairman take a much more sanguine view on the current account deficit than he take for some time\" say robert sinche head of currency strategy at bank of america in new york \"he take a longer-term view lay out a set of condition under which the current account deficit can improve this year and next\". the dollar have hit its highest level against the euro in almost three months after the federal reserve head say the us trade deficit be set to stabilise\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary_blog(\"business_003.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qGARRaca3jXw",
        "outputId": "2d12d968-c1d8-4989-9767-283711eebd85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.2874917615370608, ['yukos', 'unit', 'buyer', 'face', 'loan', 'claim']), (0.24285176610495757, ['state-owned', 'rosneft', 'buy', 'the', 'yugansk', 'unit', 'for', '$93bn', 'in', 'a', 'sale', 'force', 'by', 'russia', 'to', 'part', 'settle', 'a', '$275bn', 'tax', 'claim', 'against', 'yukos', \"yukos'\", 'owner', 'menatep', 'group', 'say', 'it', 'will', 'ask', 'rosneft', 'to', 'repay', 'a', 'loan', 'that', 'yugansk', 'have', 'secure', 'on', 'its', 'assets', 'rosneft', 'already', 'face', 'a', 'similar', '$540m', 'repayment', 'demand', 'from', 'foreign', 'bank', 'legal', 'experts', 'say', 'rosneft', 'purchase', 'of', 'yugansk', 'would', 'include', 'such', 'obligations', '\"the', 'pledge', 'assets', 'be', 'with', 'rosneft', 'so', 'it', 'will', 'have', 'to', 'pay', 'real', 'money', 'to', 'the', 'creditors', 'to', 'avoid', 'seizure', 'of', 'yugansk', 'assets\"', 'say', 'moscow-based', 'us', 'lawyer', 'jamie', 'firestone', 'who', 'be', 'not', 'connect', 'to', 'the', 'case', 'menatep', 'group', 'manage', 'director', 'tim', 'osborne', 'tell', 'the', 'reuters', 'news', 'agency', '\"if', 'they', 'default', 'we', 'will', 'fight', 'them', 'where', 'the', 'rule', 'of', 'law', 'exist', 'under', 'the', 'international', 'arbitration', 'clauses', 'of', 'the', 'credit\"']), (0.24110066284968265, ['rosneft', 'officials', 'be', 'unavailable', 'for', 'comment', 'but', 'the', 'company', 'have', 'say', 'it', 'intend', 'to', 'take', 'action', 'against', 'menatep', 'to', 'recover', 'some', 'of', 'the', 'tax', 'claim', 'and', 'debts', 'owe', 'by', 'yugansk', 'yukos', 'have', 'file', 'for', 'bankruptcy', 'protection', 'in', 'a', 'us', 'court', 'in', 'an', 'attempt', 'to', 'prevent', 'the', 'force', 'sale', 'of', 'its', 'main', 'production', 'arm', 'the', 'sale', 'go', 'ahead', 'in', 'december', 'and', 'yugansk', 'be', 'sell', 'to', 'a', 'little-known', 'shell', 'company', 'which', 'in', 'turn', 'be', 'buy', 'by', 'rosneft', 'yukos', 'claim', 'its', 'downfall', 'be', 'punishment', 'for', 'the', 'political', 'ambition', 'of', 'its', 'founder', 'mikhail', 'khodorkovsky', 'and', 'have', 'vow', 'to', 'sue', 'any', 'participant', 'in', 'the', 'sale']), (0.22855580950829876, ['the', 'owners', 'of', 'embattle', 'russian', 'oil', 'giant', 'yukos', 'be', 'to', 'ask', 'the', 'buyer', 'of', 'its', 'former', 'production', 'unit', 'to', 'pay', 'back', 'a', '$900m', '(£479m)', 'loan'])]\n",
            "Summarize Text: \n",
            " yukos unit buyer face loan claim. state-owned rosneft buy the yugansk unit for $93bn in a sale force by russia to part settle a $275bn tax claim against yukos yukos' owner menatep group say it will ask rosneft to repay a loan that yugansk have secure on its assets rosneft already face a similar $540m repayment demand from foreign bank legal experts say rosneft purchase of yugansk would include such obligations \"the pledge assets be with rosneft so it will have to pay real money to the creditors to avoid seizure of yugansk assets\" say moscow-based us lawyer jamie firestone who be not connect to the case menatep group manage director tim osborne tell the reuters news agency \"if they default we will fight them where the rule of law exist under the international arbitration clauses of the credit\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary_blog(\"business_004.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjuGle_13juF",
        "outputId": "743b6a66-6b0b-4cde-cd35-abd3d34431cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.21801766775490167, ['high', 'fuel', 'price', 'hit', 'ba', 'profit']), (0.18227174422413359, ['to', 'help', 'offset', 'the', 'increase', 'price', 'of', 'aviation', 'fuel', 'ba', 'last', 'year', 'introduce', 'a', 'fuel', 'surcharge', 'for', 'passengers']), (0.16109831580936593, ['report', 'its', 'result', 'for', 'the', 'three', 'months', 'to', '31', 'december', '2004', 'the', 'airline', 'make', 'a', 'pre-tax', 'profit', 'of', '£75m', '($141m)', 'compare', 'with', '£125m', 'a', 'year', 'earlier', 'rod', 'eddington', 'ba', 'chief', 'executive', 'say', 'the', 'result', 'be', '\"respectable\"', 'in', 'a', 'third', 'quarter', 'when', 'fuel', 'cost', 'rise', 'by', '£106m', 'or', '473%', 'ba', 'profit', 'be', 'still', 'better', 'than', 'market', 'expectation', 'of', '£59m', 'and', 'it', 'expect', 'a', 'rise', 'in', 'full-year', 'revenues']), (0.1510443229773263, ['in', 'october', 'it', 'increase', 'this', 'from', '£6', 'to', '£10', 'one-way', 'for', 'all', 'long-haul', 'flight', 'while', 'the', 'short-haul', 'surcharge', 'be', 'raise', 'from', '£250', 'to', '£4', 'a', 'leg', 'yet', 'aviation', 'analyst', 'mike', 'powell', 'of', 'dresdner', 'kleinwort', 'wasserstein', 'say', 'ba', 'estimate', 'annual', 'surcharge', 'revenues', '-', '£160m', '-', 'will', 'still', 'be', 'way', 'short', 'of', 'its', 'additional', 'fuel', 'cost', '-', 'a', 'predict', 'extra', '£250m', 'turnover', 'for', 'the', 'quarter', 'be', 'up', '43%', 'to', '£197bn', 'further', 'benefit', 'from', 'a', 'rise', 'in', 'cargo', 'revenue', 'look', 'ahead', 'to', 'its', 'full', 'year', 'result', 'to', 'march', '2005', 'ba', 'warn', 'that', 'yield', '-', 'average', 'revenues', 'per', 'passenger', '-', 'be', 'expect', 'to', 'decline', 'as', 'it', 'continue', 'to', 'lower', 'price', 'in', 'the', 'face', 'of', 'competition', 'from', 'low-cost', 'carriers', 'however', 'it', 'say', 'sales', 'would', 'be', 'better', 'than', 'previously', 'forecast', '\"for', 'the', 'year', 'to', 'march', '2005', 'the', 'total', 'revenue', 'outlook', 'be', 'slightly', 'better', 'than', 'previous', 'guidance', 'with', 'a', '3%', 'to', '35%', 'improvement', 'anticipated\"', 'ba', 'chairman', 'martin', 'broughton', 'say', 'ba', 'have', 'previously', 'forecast', 'a', '2%', 'to', '3%', 'rise', 'in', 'full-year', 'revenue']), (0.14707667557383783, ['british', 'airways', 'have', 'blame', 'high', 'fuel', 'price', 'for', 'a', '40%', 'drop', 'in', 'profit']), (0.1404912736604344, ['it', 'also', 'report', 'on', 'friday', 'that', 'passenger', 'number', 'rise', '81%', 'in', 'january', 'aviation', 'analyst', 'nick', 'van', 'den', 'brul', 'of', 'bnp', 'paribas', 'describe', 'ba', 'latest', 'quarterly', 'result', 'as', '\"pretty', 'modest\"', '\"it', 'be', 'quite', 'good', 'on', 'the', 'revenue', 'side', 'and', 'it', 'show', 'the', 'impact', 'of', 'fuel', 'surcharge', 'and', 'a', 'positive', 'cargo', 'development', 'however', 'operate', 'margins', 'down', 'and', 'cost', 'impact', 'of', 'fuel', 'be', 'very', 'strong\"', 'he', 'say', 'since', 'the', '11', 'september', '2001', 'attack', 'in', 'the', 'unite', 'state', 'ba', 'have', 'cut', '13000', 'job', 'as', 'part', 'of', 'a', 'major', 'cost-cutting', 'drive', '\"our', 'focus', 'remain', 'on', 'reduce', 'controllable', 'cost', 'and', 'debt', 'whilst', 'continue', 'to', 'invest', 'in', 'our', 'products\"', 'mr', 'eddington', 'say', '\"for', 'example', 'we', 'have', 'take', 'delivery', 'of', 'six', 'airbus', 'a321', 'aircraft', 'and', 'next', 'month', 'we', 'will', 'start', 'further', 'improvements', 'to', 'our', 'club', 'world', 'flat', 'beds\"', 'ba', 'share', 'close', 'up', 'four', 'pence', 'at', '2745', 'pence'])]\n",
            "Summarize Text: \n",
            " high fuel price hit ba profit. to help offset the increase price of aviation fuel ba last year introduce a fuel surcharge for passengers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "generate_clean_summary_blog(\"business_005.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DTVXxX7a3kDo",
        "outputId": "e784ef77-158a-4b2c-99b2-e624c151b96e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.2943405799012111, ['share', 'in', 'uk', 'drink', 'and', 'food', 'firm', 'ally', 'domecq', 'have', 'rise', 'on', 'speculation', 'that', 'it', 'could', 'be', 'the', 'target', 'of', 'a', 'takeover', 'by', 'france', 'pernod', 'ricard']), (0.24294286861867673, ['report', 'in', 'the', 'wall', 'street', 'journal', 'and', 'the', 'financial', 'time', 'suggest', 'that', 'the', 'french', 'spirit', 'firm', 'be', 'consider', 'a', 'bid', 'but', 'have', 'yet', 'to', 'contact', 'its', 'target', 'ally', 'domecq', 'share', 'in', 'london', 'rise', '4%', 'by', '1200', 'gmt', 'while', 'pernod', 'share', 'in', 'paris', 'slip', '12%', 'pernod', 'say', 'it', 'be', 'seek', 'acquisitions', 'but', 'refuse', 'to', 'comment', 'on', 'specifics']), (0.238022928422743, ['pernod', 'takeover', 'talk', 'lift', 'domecq']), (0.22469362305736895, ['pernod', 'last', 'major', 'purchase', 'be', 'a', 'third', 'of', 'us', 'giant', 'seagram', 'in', '2000', 'the', 'move', 'which', 'propel', 'it', 'into', 'the', 'global', 'top', 'three', 'of', 'drink', 'firm', 'the', 'other', 'two-thirds', 'of', 'seagram', 'be', 'buy', 'by', 'market', 'leader', 'diageo', 'in', 'term', 'of', 'market', 'value', 'pernod', '-', 'at', '75bn', 'euros', '($97bn)', '-', 'be', 'about', '9%', 'smaller', 'than', 'ally', 'domecq', 'which', 'have', 'a', 'capitalisation', 'of', '£57bn', '($107bn', '82bn', 'euros)', 'last', 'year', 'pernod', 'try', 'to', 'buy', 'glenmorangie', 'one', 'of', 'scotland', 'premier', 'whisky', 'firm', 'but', 'lose', 'out', 'to', 'luxury', 'goods', 'firm', 'lvmh', 'pernod', 'be', 'home', 'to', 'brand', 'include', 'chivas', 'regal', 'scotch', 'whisky', 'havana', 'club', 'rum', 'and', 'jacob', 'creek', 'wine', 'ally', 'domecq', 'big', 'name', 'include', 'malibu', 'rum', 'courvoisier', 'brandy', 'stolichnaya', 'vodka', 'and', 'ballantine', 'whisky', '-', 'as', 'well', 'as', 'snack', 'food', 'chain', 'such', 'as', \"dunkin'\", 'donuts', 'and', 'baskin-robbins', 'ice', 'cream', 'the', 'wsj', 'say', 'that', 'the', 'two', 'be', 'ripe', 'for', 'consolidation', 'have', 'each', 'deal', 'with', 'problematic', 'part', 'of', 'their', 'portfolio', 'pernod', 'have', 'reduce', 'the', 'debt', 'it', 'take', 'on', 'to', 'fund', 'the', 'seagram', 'purchase', 'to', 'just', '18bn', 'euros', 'while', 'ally', 'have', 'improve', 'the', 'performance', 'of', 'its', 'fast-food', 'chain'])]\n",
            "Summarize Text: \n",
            " share in uk drink and food firm ally domecq have rise on speculation that it could be the target of a takeover by france pernod ricard. report in the wall street journal and the financial time suggest that the french spirit firm be consider a bid but have yet to contact its target ally domecq share in london rise 4% by 1200 gmt while pernod share in paris slip 12% pernod say it be seek acquisitions but refuse to comment on specifics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Summarization with TF-IDF"
      ],
      "metadata": {
        "id": "qUjBPUQtDGnS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3"
      ],
      "metadata": {
        "id": "8A4bK4ZHY6DY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "from nltk import sent_tokenize, word_tokenize, PorterStemmer\n",
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "dGrCh5fjDJtW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Irol37XxEDah",
        "outputId": "ed2867cc-9f54-420b-c6c7-fb7eed78ef1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_frequency_table(text_string) -> dict:\n",
        "    \"\"\"\n",
        "    we create a dictionary for the word frequency table.\n",
        "    For this, we should only use the words that are not part of the stopWords array.\n",
        "\n",
        "    Removing stop words and making frequency table\n",
        "    Stemmer - an algorithm to bring words to its root word.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    words = word_tokenize(text_string)\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    freqTable = dict()\n",
        "    for word in words:\n",
        "        word = ps.stem(word)\n",
        "        if word in stopWords:\n",
        "            continue\n",
        "        if word in freqTable:\n",
        "            freqTable[word] += 1\n",
        "        else:\n",
        "            freqTable[word] = 1\n",
        "\n",
        "    return freqTable"
      ],
      "metadata": {
        "id": "G2EkDxvpDNOs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_frequency_matrix(sentences):\n",
        "    frequency_matrix = {}\n",
        "    stopWords = set(stopwords.words(\"english\"))\n",
        "    ps = PorterStemmer()\n",
        "\n",
        "    for sent in sentences:\n",
        "        freq_table = {}\n",
        "        words = word_tokenize(sent)\n",
        "        for word in words:\n",
        "            word = word.lower()\n",
        "            word = ps.stem(word)\n",
        "            if word in stopWords:\n",
        "                continue\n",
        "\n",
        "            if word in freq_table:\n",
        "                freq_table[word] += 1\n",
        "            else:\n",
        "                freq_table[word] = 1\n",
        "\n",
        "        frequency_matrix[sent[:15]] = freq_table\n",
        "\n",
        "    return frequency_matrix"
      ],
      "metadata": {
        "id": "f_JSfRUZDRk-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_tf_matrix(freq_matrix):\n",
        "    tf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        tf_table = {}\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, count in f_table.items():\n",
        "            tf_table[word] = count / count_words_in_sentence\n",
        "\n",
        "        tf_matrix[sent] = tf_table\n",
        "\n",
        "    return tf_matrix"
      ],
      "metadata": {
        "id": "9uBeBbQADUWl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_documents_per_words(freq_matrix):\n",
        "    word_per_doc_table = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        for word, count in f_table.items():\n",
        "            if word in word_per_doc_table:\n",
        "                word_per_doc_table[word] += 1\n",
        "            else:\n",
        "                word_per_doc_table[word] = 1\n",
        "\n",
        "    return word_per_doc_table"
      ],
      "metadata": {
        "id": "qGXeF6fsDXpW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents):\n",
        "    idf_matrix = {}\n",
        "\n",
        "    for sent, f_table in freq_matrix.items():\n",
        "        idf_table = {}\n",
        "\n",
        "        for word in f_table.keys():\n",
        "            idf_table[word] = math.log10(total_documents / float(count_doc_per_words[word]))\n",
        "\n",
        "        idf_matrix[sent] = idf_table\n",
        "\n",
        "    return idf_matrix"
      ],
      "metadata": {
        "id": "xgrtl3w1DZ9a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _create_tf_idf_matrix(tf_matrix, idf_matrix):\n",
        "    tf_idf_matrix = {}\n",
        "\n",
        "    for (sent1, f_table1), (sent2, f_table2) in zip(tf_matrix.items(), idf_matrix.items()):\n",
        "\n",
        "        tf_idf_table = {}\n",
        "\n",
        "        for (word1, value1), (word2, value2) in zip(f_table1.items(),\n",
        "                                                    f_table2.items()):  # here, keys are the same in both the table\n",
        "            tf_idf_table[word1] = float(value1 * value2)\n",
        "\n",
        "        tf_idf_matrix[sent1] = tf_idf_table\n",
        "\n",
        "    return tf_idf_matrix"
      ],
      "metadata": {
        "id": "WFSqDSVIDcsq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _score_sentences(tf_idf_matrix) -> dict:\n",
        "    \"\"\"\n",
        "    score a sentence by its word's TF\n",
        "    Basic algorithm: adding the TF frequency of every non-stop word in a sentence divided by total no of words in a sentence.\n",
        "    :rtype: dict\n",
        "    \"\"\"\n",
        "\n",
        "    sentenceValue = {}\n",
        "\n",
        "    for sent, f_table in tf_idf_matrix.items():\n",
        "        total_score_per_sentence = 0\n",
        "\n",
        "        count_words_in_sentence = len(f_table)\n",
        "        for word, score in f_table.items():\n",
        "            total_score_per_sentence += score\n",
        "\n",
        "        sentenceValue[sent] = total_score_per_sentence / count_words_in_sentence\n",
        "\n",
        "    return sentenceValue"
      ],
      "metadata": {
        "id": "ScSyZMUSDfQ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _find_average_score(sentenceValue) -> int:\n",
        "    \"\"\"\n",
        "    Find the average score from the sentence value dictionary\n",
        "    :rtype: int\n",
        "    \"\"\"\n",
        "    sumValues = 0\n",
        "    for entry in sentenceValue:\n",
        "        sumValues += sentenceValue[entry]\n",
        "\n",
        "    # Average value of a sentence from original summary_text\n",
        "    average = (sumValues / len(sentenceValue))\n",
        "\n",
        "    return average\n"
      ],
      "metadata": {
        "id": "C4l3xZARDgCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _generate_summary(sentences, sentenceValue, threshold):\n",
        "    sentence_count = 0\n",
        "    summary = ''\n",
        "\n",
        "    for sentence in sentences:\n",
        "        if sentence[:15] in sentenceValue and sentenceValue[sentence[:15]] >= (threshold):\n",
        "            summary += \" \" + sentence\n",
        "            sentence_count += 1\n",
        "\n",
        "    return summary"
      ],
      "metadata": {
        "id": "v9b-2l-hDi2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_summarization(text):\n",
        "    \"\"\"\n",
        "    :param text: Plain summary_text of long article\n",
        "    :return: summarized summary_text\n",
        "    \"\"\"\n",
        "\n",
        "    '''\n",
        "    We already have a sentence tokenizer, so we just need\n",
        "    to run the sent_tokenize() method to create the array of sentences.\n",
        "    '''\n",
        "    # 1 Sentence Tokenize\n",
        "    sentences = sent_tokenize(text)\n",
        "    total_documents = len(sentences)\n",
        "    #print(sentences)\n",
        "\n",
        "    # 2 Create the Frequency matrix of the words in each sentence.\n",
        "    freq_matrix = _create_frequency_matrix(sentences)\n",
        "    #print(freq_matrix)\n",
        "\n",
        "    '''\n",
        "    Term frequency (TF) is how often a word appears in a document, divided by how many words are there in a document.\n",
        "    '''\n",
        "    # 3 Calculate TermFrequency and generate a matrix\n",
        "    tf_matrix = _create_tf_matrix(freq_matrix)\n",
        "    #print(tf_matrix)\n",
        "\n",
        "    # 4 creating table for documents per words\n",
        "    count_doc_per_words = _create_documents_per_words(freq_matrix)\n",
        "    #print(count_doc_per_words)\n",
        "\n",
        "    '''\n",
        "    Inverse document frequency (IDF) is how unique or rare a word is.\n",
        "    '''\n",
        "    # 5 Calculate IDF and generate a matrix\n",
        "    idf_matrix = _create_idf_matrix(freq_matrix, count_doc_per_words, total_documents)\n",
        "    #print(idf_matrix)\n",
        "\n",
        "    # 6 Calculate TF-IDF and generate a matrix\n",
        "    tf_idf_matrix = _create_tf_idf_matrix(tf_matrix, idf_matrix)\n",
        "    #print(tf_idf_matrix)\n",
        "\n",
        "    # 7 Important Algorithm: score the sentences\n",
        "    sentence_scores = _score_sentences(tf_idf_matrix)\n",
        "    #print(sentence_scores)\n",
        "\n",
        "    # 8 Find the threshold\n",
        "    threshold = _find_average_score(sentence_scores)\n",
        "    #print(threshold)\n",
        "\n",
        "    # 9 Important Algorithm: Generate the summary\n",
        "    summary = _generate_summary(sentences, sentence_scores, 1.3 * threshold)\n",
        "    return summary"
      ],
      "metadata": {
        "id": "CAsfRA_TDqKW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_str = '''\n",
        "Those Who Are Resilient Stay In The Game Longer\n",
        "“On the mountains of truth you can never climb in vain: either you will reach a point higher up today, or you will be training your powers so that you will be able to climb higher tomorrow.” — Friedrich Nietzsche\n",
        "Challenges and setbacks are not meant to defeat you, but promote you. However, I realise after many years of defeats, it can crush your spirit and it is easier to give up than risk further setbacks and disappointments. Have you experienced this before? To be honest, I don’t have the answers. I can’t tell you what the right course of action is; only you will know. However, it’s important not to be discouraged by failure when pursuing a goal or a dream, since failure itself means different things to different people. To a person with a Fixed Mindset failure is a blow to their self-esteem, yet to a person with a Growth Mindset, it’s an opportunity to improve and find new ways to overcome their obstacles. Same failure, yet different responses. Who is right and who is wrong? Neither. Each person has a different mindset that decides their outcome. Those who are resilient stay in the game longer and draw on their inner means to succeed.\n",
        "\n",
        "I’ve coached mummy and mom clients who gave up after many years toiling away at their respective goal or dream. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It was the 19th Century’s minister Henry Ward Beecher who once said: “One’s best success comes after their greatest disappointments.” No one knows what the future holds, so your only guide is whether you can endure repeated defeats and disappointments and still pursue your dream. Consider the advice from the American academic and psychologist Angela Duckworth who writes in Grit: The Power of Passion and Perseverance: “Many of us, it seems, quit what we start far too early and far too often. Even more than the effort a gritty person puts in on a single day, what matters is that they wake up the next day, and the next, ready to get on that treadmill and keep going.”\n",
        "\n",
        "I know one thing for certain: don’t settle for less than what you’re capable of, but strive for something bigger. Some of you reading this might identify with this message because it resonates with you on a deeper level. For others, at the end of their tether the message might be nothing more than a trivial pep talk. What I wish to convey irrespective of where you are in your journey is: NEVER settle for less. If you settle for less, you will receive less than you deserve and convince yourself you are justified to receive it.\n",
        "\n",
        "\n",
        "“Two people on a precipice over Yosemite Valley” by Nathan Shipps on Unsplash\n",
        "Develop A Powerful Vision Of What You Want\n",
        "“Your problem is to bridge the gap which exists between where you are now and the goal you intend to reach.” — Earl Nightingale\n",
        "I recall a passage my father often used growing up in 1990s: “Don’t tell me your problems unless you’ve spent weeks trying to solve them yourself.” That advice has echoed in my mind for decades and became my motivator. Don’t leave it to other people or outside circumstances to motivate you because you will be let down every time. It must come from within you. Gnaw away at your problems until you solve them or find a solution. Problems are not stop signs, they are advising you that more work is required to overcome them. Most times, problems help you gain a skill or develop the resources to succeed later. So embrace your challenges and develop the grit to push past them instead of retreat in resignation. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? Are you willing to play bigger even if it means repeated failures and setbacks? You should ask yourself these questions to decide whether you’re willing to put yourself on the line or settle for less. And that’s fine if you’re content to receive less, as long as you’re not regretful later.\n",
        "\n",
        "If you have not achieved the success you deserve and are considering giving up, will you regret it in a few years or decades from now? Only you can answer that, but you should carve out time to discover your motivation for pursuing your goals. It’s a fact, if you don’t know what you want you’ll get what life hands you and it may not be in your best interest, affirms author Larry Weidel: “Winners know that if you don’t figure out what you want, you’ll get whatever life hands you.” The key is to develop a powerful vision of what you want and hold that image in your mind. Nurture it daily and give it life by taking purposeful action towards it.\n",
        "\n",
        "Vision + desire + dedication + patience + daily action leads to astonishing success. Are you willing to commit to this way of life or jump ship at the first sign of failure? I’m amused when I read questions written by millennials on Quora who ask how they can become rich and famous or the next Elon Musk. Success is a fickle and long game with highs and lows. Similarly, there are no assurances even if you’re an overnight sensation, to sustain it for long, particularly if you don’t have the mental and emotional means to endure it. This means you must rely on the one true constant in your favour: your personal development. The more you grow, the more you gain in terms of financial resources, status, success — simple. If you leave it to outside conditions to dictate your circumstances, you are rolling the dice on your future.\n",
        "\n",
        "So become intentional on what you want out of life. Commit to it. Nurture your dreams. Focus on your development and if you want to give up, know what’s involved before you take the plunge. Because I assure you, someone out there right now is working harder than you, reading more books, sleeping less and sacrificing all they have to realise their dreams and it may contest with yours. Don’t leave your dreams to chance.\n",
        "'''"
      ],
      "metadata": {
        "id": "Sxbi3dmND-ma"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = run_summarization(text_str)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D-50W5-uDtF3",
        "outputId": "90170340-b0c0-4ea4-ec06-b0464f34a5d9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Have you experienced this before? Who is right and who is wrong? Neither. It was at that point their biggest breakthrough came. Perhaps all those years of perseverance finally paid off. It must come from within you. Where are you settling in your life right now? Could you be you playing for bigger stakes than you are? So become intentional on what you want out of life. Commit to it. Nurture your dreams.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Compare similarity matrix and TF-IDF methods"
      ],
      "metadata": {
        "id": "ta1E9H2VFeNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#similarity matrix\n",
        "generate_clean_summary_blog(\"business_001.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFEqVKpGFd-R",
        "outputId": "bc4a2600-c250-401d-dc69-a3334bc90e9b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.21293950793362676, ['the', 'firm', 'which', 'be', 'now', 'one', 'of', 'the', 'biggest', 'investors', 'in', 'google', 'benefit', 'from', 'sales', 'of', 'high-speed', 'internet', 'connections', 'and', 'higher', 'advert', 'sales', 'timewarner', 'say', 'fourth', 'quarter', 'sales', 'rise', '2%', 'to', '$111bn', 'from', '$109bn', 'its', 'profit', 'be', 'buoy', 'by', 'one-off', 'gain', 'which', 'offset', 'a', 'profit', 'dip', 'at', 'warner', 'bros', 'and', 'less', 'users', 'for', 'aol']), (0.2091791279867516, ['time', 'warner', 'say', 'on', 'friday', 'that', 'it', 'now', 'own', '8%', 'of', 'search-engine', 'google', 'but', 'its', 'own', 'internet', 'business', 'aol', 'have', 'have', 'mix', 'fortunes', 'it', 'lose', '464000', 'subscribers', 'in', 'the', 'fourth', 'quarter', 'profit', 'be', 'lower', 'than', 'in', 'the', 'precede', 'three', 'quarter', 'however', 'the', 'company', 'say', 'aol', 'underlie', 'profit', 'before', 'exceptional', 'items', 'rise', '8%', 'on', 'the', 'back', 'of', 'stronger', 'internet', 'advertise', 'revenues', 'it', 'hop', 'to', 'increase', 'subscribers', 'by', 'offer', 'the', 'online', 'service', 'free', 'to', 'timewarner', 'internet', 'customers', 'and', 'will', 'try', 'to', 'sign', 'up', 'aol', 'exist', 'customers', 'for', 'high-speed', 'broadband', 'timewarner', 'also', 'have', 'to', 'restate', '2000', 'and', '2003', 'result', 'follow', 'a', 'probe', 'by', 'the', 'us', 'securities', 'exchange', 'commission', '(sec)', 'which', 'be', 'close', 'to', 'conclude']), (0.18594225357018157, ['time', 'warner', 'fourth', 'quarter', 'profit', 'be', 'slightly', 'better', 'than', \"analysts'\", 'expectations', 'but', 'its', 'film', 'division', 'saw', 'profit', 'slump', '27%', 'to', '$284m', 'help', 'by', 'box-office', 'flop', 'alexander', 'and', 'catwoman', 'a', 'sharp', 'contrast', 'to', 'year-earlier', 'when', 'the', 'third', 'and', 'final', 'film', 'in', 'the', 'lord', 'of', 'the', 'ring', 'trilogy', 'boost', 'result', 'for', 'the', 'full-year', 'timewarner', 'post', 'a', 'profit', 'of', '$336bn', 'up', '27%', 'from', 'its', '2003', 'performance', 'while', 'revenues', 'grow', '64%', 'to', '$4209bn', '\"our', 'financial', 'performance', 'be', 'strong', 'meet', 'or', 'exceed', 'all', 'of', 'our', 'full-year', 'objectives', 'and', 'greatly', 'enhance', 'our', 'flexibility\"', 'chairman', 'and', 'chief', 'executive', 'richard', 'parsons', 'say', 'for', '2005', 'timewarner', 'be', 'project', 'operate', 'earn', 'growth', 'of', 'around', '5%', 'and', 'also', 'expect', 'higher', 'revenue', 'and', 'wider', 'profit', 'margins']), (0.17551083773768114, ['ad', 'sales', 'boost', 'time', 'warner', 'profit']), (0.1263663228982152, ['quarterly', 'profit', 'at', 'us', 'media', 'giant', 'timewarner', 'jump', '76%', 'to', '$113bn', '(£600m)', 'for', 'the', 'three', 'months', 'to', 'december', 'from', '$639m', 'year-earlier']), (0.09006194987354366, ['timewarner', 'be', 'to', 'restate', 'its', 'account', 'as', 'part', 'of', 'efforts', 'to', 'resolve', 'an', 'inquiry', 'into', 'aol', 'by', 'us', 'market', 'regulators', 'it', 'have', 'already', 'offer', 'to', 'pay', '$300m', 'to', 'settle', 'charge', 'in', 'a', 'deal', 'that', 'be', 'under', 'review', 'by', 'the', 'sec', 'the', 'company', 'say', 'it', 'be', 'unable', 'to', 'estimate', 'the', 'amount', 'it', 'need', 'to', 'set', 'aside', 'for', 'legal', 'reserve', 'which', 'it', 'previously', 'set', 'at', '$500m', 'it', 'intend', 'to', 'adjust', 'the', 'way', 'it', 'account', 'for', 'a', 'deal', 'with', 'german', 'music', 'publisher', 'bertelsmann', 'purchase', 'of', 'a', 'stake', 'in', 'aol', 'europe', 'which', 'it', 'have', 'report', 'as', 'advertise', 'revenue', 'it', 'will', 'now', 'book', 'the', 'sale', 'of', 'its', 'stake', 'in', 'aol', 'europe', 'as', 'a', 'loss', 'on', 'the', 'value', 'of', 'that', 'stake'])]\n",
            "Summarize Text: \n",
            " the firm which be now one of the biggest investors in google benefit from sales of high-speed internet connections and higher advert sales timewarner say fourth quarter sales rise 2% to $111bn from $109bn its profit be buoy by one-off gain which offset a profit dip at warner bros and less users for aol. time warner say on friday that it now own 8% of search-engine google but its own internet business aol have have mix fortunes it lose 464000 subscribers in the fourth quarter profit be lower than in the precede three quarter however the company say aol underlie profit before exceptional items rise 8% on the back of stronger internet advertise revenues it hop to increase subscribers by offer the online service free to timewarner internet customers and will try to sign up aol exist customers for high-speed broadband timewarner also have to restate 2000 and 2003 result follow a probe by the us securities exchange commission (sec) which be close to conclude\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf\n",
        "with open('business_001.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "result = run_summarization(data)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZU4PL8w9E2WL",
        "outputId": "68228d5e-4fed-4dbe-8fdd-da270976e70d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " But its own internet business, AOL, had has mixed fortunes. It lost 464,000 subscribers in the fourth quarter profits were lower than in the preceding three quarters. It will now book the sale of its stake in AOL Europe as a loss on the value of that stake.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#similarity matrix\n",
        "generate_clean_summary_blog(\"business_005.txt\", 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GJyqmKapGGZJ",
        "outputId": "2e45e494-d867-4b6c-dc73-7faa0b751005"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Indexes of top ranked_sentence order are  [(0.2943405799012111, ['share', 'in', 'uk', 'drink', 'and', 'food', 'firm', 'ally', 'domecq', 'have', 'rise', 'on', 'speculation', 'that', 'it', 'could', 'be', 'the', 'target', 'of', 'a', 'takeover', 'by', 'france', 'pernod', 'ricard']), (0.24294286861867673, ['report', 'in', 'the', 'wall', 'street', 'journal', 'and', 'the', 'financial', 'time', 'suggest', 'that', 'the', 'french', 'spirit', 'firm', 'be', 'consider', 'a', 'bid', 'but', 'have', 'yet', 'to', 'contact', 'its', 'target', 'ally', 'domecq', 'share', 'in', 'london', 'rise', '4%', 'by', '1200', 'gmt', 'while', 'pernod', 'share', 'in', 'paris', 'slip', '12%', 'pernod', 'say', 'it', 'be', 'seek', 'acquisitions', 'but', 'refuse', 'to', 'comment', 'on', 'specifics']), (0.238022928422743, ['pernod', 'takeover', 'talk', 'lift', 'domecq']), (0.22469362305736895, ['pernod', 'last', 'major', 'purchase', 'be', 'a', 'third', 'of', 'us', 'giant', 'seagram', 'in', '2000', 'the', 'move', 'which', 'propel', 'it', 'into', 'the', 'global', 'top', 'three', 'of', 'drink', 'firm', 'the', 'other', 'two-thirds', 'of', 'seagram', 'be', 'buy', 'by', 'market', 'leader', 'diageo', 'in', 'term', 'of', 'market', 'value', 'pernod', '-', 'at', '75bn', 'euros', '($97bn)', '-', 'be', 'about', '9%', 'smaller', 'than', 'ally', 'domecq', 'which', 'have', 'a', 'capitalisation', 'of', '£57bn', '($107bn', '82bn', 'euros)', 'last', 'year', 'pernod', 'try', 'to', 'buy', 'glenmorangie', 'one', 'of', 'scotland', 'premier', 'whisky', 'firm', 'but', 'lose', 'out', 'to', 'luxury', 'goods', 'firm', 'lvmh', 'pernod', 'be', 'home', 'to', 'brand', 'include', 'chivas', 'regal', 'scotch', 'whisky', 'havana', 'club', 'rum', 'and', 'jacob', 'creek', 'wine', 'ally', 'domecq', 'big', 'name', 'include', 'malibu', 'rum', 'courvoisier', 'brandy', 'stolichnaya', 'vodka', 'and', 'ballantine', 'whisky', '-', 'as', 'well', 'as', 'snack', 'food', 'chain', 'such', 'as', \"dunkin'\", 'donuts', 'and', 'baskin-robbins', 'ice', 'cream', 'the', 'wsj', 'say', 'that', 'the', 'two', 'be', 'ripe', 'for', 'consolidation', 'have', 'each', 'deal', 'with', 'problematic', 'part', 'of', 'their', 'portfolio', 'pernod', 'have', 'reduce', 'the', 'debt', 'it', 'take', 'on', 'to', 'fund', 'the', 'seagram', 'purchase', 'to', 'just', '18bn', 'euros', 'while', 'ally', 'have', 'improve', 'the', 'performance', 'of', 'its', 'fast-food', 'chain'])]\n",
            "Summarize Text: \n",
            " share in uk drink and food firm ally domecq have rise on speculation that it could be the target of a takeover by france pernod ricard. report in the wall street journal and the financial time suggest that the french spirit firm be consider a bid but have yet to contact its target ally domecq share in london rise 4% by 1200 gmt while pernod share in paris slip 12% pernod say it be seek acquisitions but refuse to comment on specifics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#tf-idf\n",
        "with open('business_005.txt', 'r') as file:\n",
        "    data = file.read()\n",
        "\n",
        "result = run_summarization(data)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXyNtypnGG5J",
        "outputId": "c6ac719f-9953-4070-e5dc-dd738e85fba6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Pernod said it was seeking acquisitions but refused to comment on specifics. The other two-thirds of Seagram was bought by market leader Diageo. The WSJ said that the two were ripe for consolidation, having each dealt with problematic parts of their portfolio.\n"
          ]
        }
      ]
    }
  ]
}